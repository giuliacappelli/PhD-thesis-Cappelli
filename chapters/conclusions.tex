% \setchapterimage[6.5cm]{seaside}
\setchapterpreamble[u]{\margintoc}
\chapter{Conclusions and open questions}
\labch{conclusions}

% Conclusions

\section{Final comments} \labsec{end_discussion}

\subsection{Recap of main findings}

I presented my models of the indefinite implicit object construction in English and in Italian in \refch{model}, together with a discussion of their performance and the results they yielded. Here I will provide a short summary of the main findings of the two full models, namely, the five-predictor models I computed using Behavioral PISA (introduced in \refsec{behavPisa}) to quantify semantic selectivity, as explained in \refsec{intro_models}.\\
In general, they perform comparably well, explaining almost half of the variance in the data (adjusted R\textsuperscript{2} is 0.468 for the English model and 0.455 for the Italian model, as reported in \refsec{intro_evaluation}). As I argued in \refch{model}, the better performance of the English model with respect to the Italian model may depend on semantic differences in the senses of the target verbs included in the behavioral experiments, and also on a more clear-cut role the predictors play in English grammar than in Italian grammar. These results, especially in the light of the fact that the full models improve on the performance of the reduced models (namely, Medina's three-predictor model and my four-predictor model not considering manner specification), are indeed encouraging. However, even the full models fall short of explaining all the variance in the data, demonstrating that there is much room for improvement. I will propose some ideas to expand upon these models and improve their computation in future research in \refsec{end_future}.\\
Let us look more closely at the constraint re-rankings and subsequent grammaticality predictions of object drop in the two models. Both in English and in Italian the probability of \textsc{*Int Arg} outranking \textsc{Telic End}\sidenote{Required to have grammatical implicit indefinite objects with telic verbs, as explained in \refch{medina} and \refch{model}.} varies strongly depending on Behavioral PISA, so that the curves described by the functions associated to this probability in the two languages are the steepest among all the curves associated to the re-ranking probabilities (as shown in \refsec{stot_full_parameters}). However, while in English the probability of \textsc{*Int Arg} outranking \textsc{Telic End} is directly proportional to semantic selectivity, in Italian their relation is one of inverse proportionality, for reasons I discussed when commenting \reffig{ita_bpisa_telicity}.\\
Moreover, while in both languages the probability of \textsc{*Int Arg} outranking \textsc{Telic End} varies greatly depending on Behavioral PISA, there are differences with respect to the other predictors. In particular, in English there is an interaction between the functions associated to the re-ranking probabilities of \textsc{Telic End} and \textsc{Perf Coda}, so that telic imperfective verbs are more likely to drop their object than atelic perfective verbs for high Behavioral PISA values, while the opposite holds for Behavioral PISA scores lower than 0.8, approximately. In Italian, instead, there is an interaction between the function associated to the re-ranking probability of \textsc{Mann-Spec Arg} and the functions associated to the re-ranking probabilities of all the other constraints\sidenote{The interaction with \textsc{Non-Iter Arg} is trivial, because it happens when Behavioral PISA is 1, i.e., the maximum value.}. Thus, both models show a main effect of telicity on the probability the object-less use of a transitive verb will be considered grammatical, but the second most relevant factor in the model is perfectivity in English and manner specification in Italian.\\
In addition to the adjusted R\textsuperscript{2} values and the main role played by telicity, the two models are also comparable for the range of predicted object-dropping probabilities (30-100\% in English, 30-90\% in Italian), for the relevance of semantic selectivity in determining the grammaticality of object drop (with re-ranking probabilities that are always directly proportional to Behavioral PISA, with the exception of \textsc{Telic End} in Italian), and for the fact that the predictors perform consistently with theoretical literature on object drop (refer to \refch{objectdrop}, \refch{factors}, and \refch{predictors}). Indeed, in both models atelic imperfective iterative manner-specified inputs are the most likely to drop their object (between 80\% and 90\%), while telic perfective non-iterative manner-unspecified inputs are the least likely (between 30\% and 40\%). Moreover, atelic inputs are more likely to occur with implicit objects than telic inputs, imperfective inputs more than perfective inputs, iterative inputs more than non-iterative inputs, and manner-unspecified inputs more than manner-specified inputs, as expected.\\
Even though semantic selectivity plays an active role in both models, the range of predicted grammaticality across different input types is not the same in English and in Italian. Indeed, while it is comparable for low-Behavioral PISA verbs in the two languages (ranging from 30\% to 80\% in English and from 30\% to 90\% in Italian), it is much narrower and higher in English (between 90\% and 100\%) than in Italian (between 40\% and 90\%), as shown and discussed in \refsec{stot_full_predicted}.


\subsection{Comments on iterativity and manner specification}

In this dissertation I modeled the indefinite implicit object construction following \posscite{Medina2007} steps. A major element of novelty I added is the inclusion of two novel constraints in the model, i.e., \textsc{Non-Iter Arg} and \textsc{Mann-Spec Arg} (refer to \refch{model}), which are based on the role played by iterativity and manner specification (refer to \refch{factors}), respectively, in facilitating indefinite object drop.\\
As I argued in \refsec{intro_evaluation}, the addition of these two new constraints proved to be beneficial to the performance of the model when applied both to English and to Italian data. Indeed, the full five-predictor models explain the variance in the data better than the three- and the four-predictor models regardless of the chosen measure of semantic selectivity (Resnik's SPS, Computational PISA, or Behavioral PISA). However, the four-predictor models (including iterativity in addition to Medina's telicity, perfectivity, and semantic selectivity) do not perform better than the three-predictor models, with basically identical adjusted R\textsuperscript{2} values in English and slightly smaller adjusted R\textsuperscript{2} values in Italian. Taken together, these results mean that iterativity alone is not a sufficient addition to Medina's model (rather, it makes the model needlessly more complicated, since it does not explain more variance in the data), but models including iterativity and manner specification together have a noticeably stronger explanatory power.\\
The disappointing performance of the models including iterativity without manner specification echoes observations drawn in \refsec{stot_full_parameters} relative to the probability of \textsc{*Int Arg} outranking \textsc{Non-Iter Arg}, that was shown to be very high both in a relative sense (since it is the highest among all the re-ranking probabilities) and in an absolute sense (92-100\% in English, 96-100\% in Italian). Since \textsc{Non-Iter Arg} is vacuously satisfied by iterative inputs, and it varies almost imperceptibly according to semantic selectivity with non-iterative inputs (for which it is an active constraint), it stands to reason that it has no noticeable effect on the predicted grammaticality of object drop.\\
The same analysis would also explain the significant effect of the full models, where manner specification is also included among the predictors. In particular, in \refsec{intro_evaluation} I showed that the addition of manner specification determines a much stronger qualitative leap in the full models of Italian than in English, where the increase in the performance of the models is rather modest. Once again, these results are related to the probability of \textsc{*Int Arg} outranking the relevant constraint, i.e., \textsc{Mann-Spec Arg}. As shown in \refsec{stot_full_parameters}, the curve described by the function associated to this re-ranking probability is quite steep in Italian and it intersects all the other curves, while in English it is not steep at all and it has no interactions with the other curves. Thus, manner specification in Italian interacts in meaningful ways with semantic selectivity (as shown by the steepness of the curve) and the other binary predictors of object drop, making it an important factor to include in an expanded model of indefinite object drop. In English it has an effect too, but it is less evident.


\subsection{Comments on semantic selectivity}

In addition to the presence of additional constraints in the models, the other dimension of variation highlighted in \reftab{tab_mymodels} in \refsec{intro_models} is the measure used to quantify semantic selectivity in the models. In particular, I computed three families of models, each based on Resnik's SPS (as in Medina's original model), on Computational PISA, or on Behavioral PISA. In \refsec{intro_evaluation} I provided adjusted R\textsuperscript{2} scores of each model in each family, showing that in English SPS-based models are the worst-performing, while PISA-based models are noticeably better (with Behavioral PISA being better than Computational PISA). In Italian, instead, Computational PISA-based models are the worst-performing among all, followed by SPS-based models and, lastly, by Behavioral PISA-based models.\\
In \refch{model} I provided a possible explanation of these facts, together with the correlations between semantic selectivity and average Likert grammaticality ratings presented in \refsec{eng_judgresult_semsel} and \refsec{ita_judgresult_semsel}, by making reference to the way each measure of semantic selectivity is defined and computed (detailed in \refsec{predictor_sps}). Let us consider the different performance of the three families of models in the light of \refsec{evalMySPSs}, where I evaluated the correlations between the three measures of semantic selectivity both in English and in Italian. I argued that the English facts (SPS-based models being worse than PISA-based models, and SPS correlating poorly with PISAs while Behavioral PISA and Computational PISA correlate well with each other) may depend on the nature of these measures, considering that both PISA measures are based on the computation of pairwise similarity scores (distributional cosine similarity for Computational PISA, Likert-scale human judgments of similarity for Behavioral PISA), while SPS suffers from all the problems of taxonomy-based measures, as discussed in \refsec{predictor_sps}. The Italian picture is different, since in this case Computational PISA-based models perform worse than SPS-based models. Interestingly, as shown in \refsec{ita_judgresult_semsel}, Computational PISA in Italian correlates very well with SPS (even better than with Behavioral PISA). I take this to mean that even after the manual cleansing I performed to purge any artifacts from the corpus data (recounted and motivated on \refpage{cleanthecorpus}), the itWaC corpus, upon which I based the computation of SPS and Computational PISA relative to Italian, has a stronger effect on the resulting scores than the ukWaC corpus, which I used to model the computational measures of semantic selectivity in English.\\
However, there may also be some undesirable side effect generated by the choice of ukWaC for English, given that I was able to reproduce Medina's findings relative to indefinite object drop in English with my three-predictor PISA-based models, but not with SPS, which is the measure she herself used (refer to \refsec{concl_medinacompare}). This may depend either on the corpus of choice or on the set of target verbs (or, why not, on both), but I am confident I can take the verbs off the suspect list because I made sure to include strictly transitive, as much as possible monosemous, verbs in my set (refer to \refsec{verbs}), while Medina used the same verbs included in \posscite{Resnik1993} original computation of SPS, a set including also verbs such as \textit{to do, to get, to have}. Thus, I would conclude that I was not able to reproduce Medina's model using SPS because of the corpus I used. It would be possible to test this hypothesis by computing again Medina's model using her verb set and ukWaC, and my verb set and the corpus Resnik (and thus Medina) based his computation on. If my hypothesis is correct, I would not replicate Medina's results in the former scenario, but I would do so in the latter. Of course, one may argue that the fault in the lack of replication may well lie in Medina's model itself, namely, either in her computation of SPS, or in artifacts determined by her choice of target verbs (which may be poor representatives of optionally transitive verbs in English). However, she based her analysis on Resnik's own verb set and SPS scores, and it would be very improbable indeed that the inventor of SPS himself miscalculated his own model of object recoverability. The verbs would be a more likely culprit, but I was able to replicate Medina's results with my PISA-based three-predictor models. How could Medina's model be wrong, but also consistent with a model of object drop (and, consequently, of the grammar of the English language) based on a different measure of semantic selectivity and different target verbs? Clearly, this scenario would be most unlikely.\\
To conclude, I observe that both in English and in Italian the best-performing models are based on Behavioral PISA. This result does not surprise at all, since this measure, due to being based on human similarity judgments, can be considered a benchmark model of semantic selectivity.


\section{Future directions} \labsec{end_future}

\subsection{Expanding the model} \labsec{expandingthemodel}

\paragraph{Additional predictors}

Among the linguistic factors facilitating indefinite object drop that I presented in \refch{factors}, I only picked five to serve as predictors in my models (detailed in \refch{predictors}). These are semantic selectivity, telicity, perfectivity (all three from \posscite{Medina2007} original model), manner specification, and iterativity (two novel additions), for reasons detailed in \refsec{factorsofchoice}. The way these predictors are implemented in the (linear) Stochastic Optimality Theoretic models was explained in \refch{medina} relative to Medina's model and in \refch{model} relative to mine.\\
Future research may expand upon my models in the same way I expanded upon Medina's, namely, by introducing additional predictors in the model based on theoretical literature. A relevant area of interest, which I only brushed against by including iterativity (a broadly-intended pragmatic factor) in my model, is that of pragmatic and discourse factors (refer to \refsec{pragmaticfactors}). Since context-free utterances only happen in laboratory environments, research on pragmatic factors and the influence of extra-linguistic context will provide much more ecological data to studies on indefinite object drop. However, this should not be intended as a potshot at models based on no-context stimuli, such as Medina's and the ones I proposed in this dissertation. Indeed, given that the same semantic and aspectual factors determine indefinite object drop both in context-rich and in no-context utterances, it makes sense to model these factors first and to add contextual factors later on.\\
Given that indefinite object drop challenges prototypical transitivity, it would also be interesting to include in the model the neglected parameters described by \textcite{HopperThompson1980} (refer back to \reftab{ht1980_parameters} in \refsec{theory_transitivity}), in particular affirmation, mode, agency (strictly related to Agent affectedness, discussed on \refpage{affectedagent} and in \refsec{agentaffect}), and affectedness of the object (tackled in \refch{objectdrop}).

\paragraph{Corpus frequencies}

As shown in \textcite{Boersma2004, BoersmaHayes2001empirical}, Stochastic Optimality Theory can be used to model corpus frequencies just as well as grammaticality judgments, namely, via the evaluation of constraints that get re-ranked along a continuous numerical scale (refer to \refch{modeltheory}). Rather than trivially duplicating the results I obtained and discussed in this dissertation, new models of corpus frequencies are sure to shed a different light on the indefinite implicit object construction. As I anticipated in \refsec{frequencyfail}, neither \textcite{Resnik1993, Resnik1996} nor \textcite{Medina2007} found a precise correlation between corpus frequencies and gradient grammaticality judgments provided in behavioral experiments about indefinite implicit objects.\\
Indeed, linguistic research has long since shown that there is no clear-cut correspondence between ratings elicited from native speakers and corpus frequencies \parencite{manning2003probabilistic}. In particular, it is often the case that low-frequency utterances (or other linguistic items) receive mid-to-high acceptability judgments in behavioral experiments \parencite{KempenHarbusch2005, BermelKnittl2012, BaderHaussler2010, Boersma2004, KellerAsudeh2002}. There is also no strict relation between the relative grammaticality of a linguistic structure with respect to another and their relative corpus frequencies, since, for instance, \textcite[315-316]{BaderHaussler2010} report that they found no pairs of syntactic structures in their study where a member of the pair was judged as more grammatical than the other but occurred with a smaller frequency in the corpus, while \textcite{Boersma2004} argues in favor of the opposite. Moreover, \textcite{BaderHaussler2010} experimental results show both a "ceiling mismatch"\sidenote{The authors also observe that this is not a measurement artifact due to the use of a capped scale, such as binary or 7-point Likert ratings, because it is also found with Magnitude Estimation ratings, which are open-ended both at the top and at the bottom.} (meaning that two syntactic structures may be judged as maximally grammatical, but occur with different frequencies in the corpus) and a "floor mismatch" (meaning that two syntactic structures may never or almost-never occur in the corpus, but receive different acceptability judgments).\\
A common worry about linguistic research based on corpus material is that frequencies are less reliable than human judgments because there is no way to control language production as one controls an experimental design. This line of reasoning would surely curb easy enthusiasm about the replication of the current study to model corpus frequencies of indefinite object drop, if \textcite{Steube2008, Schutze2016} did not observe that acceptability ratings are too "contaminated by performance factors", that is to say, biased by other tasks the raters perform in addition to the one they are explicitly asked to carry out (e.g., they judge the similarity between the target sentence and the one they consider its "ideal delivery" paraphrase). Thus, if linguistics gladly relies on acceptability judgments (and, oftentimes, the results of one's own introspection), provided they are based on a rigorous experimental design, there should be no qualms about modeling corpus frequencies, provided they are interpreted in the light of the factors possibly influencing them.\\
Modeling corpus frequencies of indefinite null objects using the very same model(s) I defined in this dissertation may present additional challenges if compared to modeling acceptability ratings, since it is impossible to manipulate aspectual and discourse factors in a corpus study as in a behavioral experiment. However, the possible absence (or very low frequency) of a given object-less verb in a given aspect may well be considered an interesting, modelable datum in itself, provided one adjusts the model to account for such findings. Alternatively, it would be possible to design a production experiment to design an \textit{ad-hoc} corpus to model the frequency of indefinite null objects in controlled speech or writing. It is also important to note that it would be possible, if not even easy, to include discourse and world-knowledge context (somewhat ancillary to semantics and aspectual factors in this dissertation) in a model of object drop based on frequencies extracted from a large corpus, given that these null objects appear in sentences which are part of larger documents with explicit context information. Moreover, a corpus study of object drop may provide an answer to a question foreshadowed by \textcite{KempenHarbusch2005, Medina2007} (refer to \refsec{frequencyfail}), namely, whether a "production threshold" exists blocking mid-to-low grammaticality structures from ever being uttered and, if so, which numerical value has to be assigned to this threshold.


\paragraph{Other implicit complements of verbs}
Direct object of optionally transitive verbs are far from being the only NP complements of verbs participating in syntactic omissions. For instance, literature mentions:

\begin{itemize}
    \item Agents of passives, as in \textit{The ship was sunk $\varnothing$\textsubscript{Agent}} \parencite{BhattPancheva2017implicit, Lasersohn1993, RuppenhoferMichaelis2014};
    \item Source \parencite{Gillon2006english}, Goal \parencite{Lasersohn1993, RuppenhoferMichaelis2014}, and Path \parencite{recanati2002unarticulated} locative phrases occurring with motion verbs\sidenote{Interestingly, \textcite[9-12]{Gillon2006english} and \textcite[333]{ruppenhofer2011search} observe that in some pairs of near-synonym verbs, such as \textit{to leave, to vacate} and \textit{to arrive, to reach}, only one member of the pair allows for the omission of the locative phrase. This is consistent with the literature on the role of manner specification in argument omission (refer to \refsec{mannerspec}).}, as in \textit{Bill left $\varnothing$\textsubscript{Source}}, \textit{Hilary arrived $\varnothing$\textsubscript{Goal}}, and  \textit{The cow jumped over $\varnothing$\textsubscript{Path}};
    \item Themes of reflexive (\textit{Peter shaved (himself)}) and reciprocal (\textit{Mary and Peter divorced (from each other)}) predicates \parencite{NemethEniko2014};
    \item Recipients of three-argument verbs, as in \textit{The mayor donated \$300 $\varnothing$\textsubscript{Recipient}} \parencite{ruppenhofer2005regularities};
    \item Instruments, as in \textit{The executioner beheaded the prisoner $\varnothing$\textsubscript{Instrument}} \parencite{KoenigEtAl2002, KoenigEtAl2003, KoenigEtAl2007, RissmanEtAl2015, RissmanRawlins2017, Rissman2010}.
\end{itemize}

Among all these \textit{syntactically} optional complements of verbs, Instruments stand out because they can also be \textit{semantically} optional. Indeed, \textcite{KoenigEtAl2002, KoenigEtAl2003, KoenigEtAl2007} divided Instrument-taking verbs into two classes, i.e., the Require-Instrument class (\textit{to chop, to slice, to write}) and the Allow-Instrument class (\textit{to eat, to break, to open}). In \textcite{CappelliLenciPISA}, I computed Computational PISA scores of Instrument-taking English verbs (together with transitive verbs, as discussed in \refsec{compuPisa}), showing that this measure of semantic selectivity can reliably tell apart Require- and Allow-Instrument verbs. I argue that this is a promising starting point in a possible computational model of the factors regulating the syntactic optionality of Instruments, given the insight this method provided in the study of indefinite object drop.\\
Modeling Instruments, as well as the other implicit complements listed in this Section, will provide useful information to further theoretical and experimental research on syntactic optionality.

\paragraph{Indefinite object drop diachronically}
As argued in \refsec{frequencyfail}, with reference to \textcite{Goldberg2001, Goldberg2005a, Lorenzetti2008, Glass2020}, verbs appearing in generic contexts with a habitual interpretation (e.g., \textit{Pat drinks; Pat smokes; Chris sings; Sam bakes}, from \textcite[518]{Goldberg2001}) are likely to participate in the indefinite implicit object construction, due to Goldberg's principle of Omission under Low Discourse Prominence. Diachronically, the frequent use of transitive verbs in such contexts probably led to the grammaticalization of their intransitive use in episodic contexts as well, often with a specialized meaning (e.g. 'to drink alcohol' in \textit{Pat drinks}, 'to bake pastries' in \textit{Sam bakes}).\\
A model of indefinite object drop in historical and contemporary texts (a gap in the literature first observed by \textcite{Goldberg2001}) would substantiate this hypothesis and shed some more light on the mechanisms regulating the role of semantic and aspectual predictors in addition to discourse factors.

\paragraph{Typologically different languages}
In this dissertation, I modeled the indefinite implicit object construction in English and Italian, two typologically close languages. As discussed in \refch{model}, several differences between the two emerged with respect to their licensing of indefinite null objects, meaning that this phenomenon depends on much finer-grained aspects than what typology alone would warrant. Nevertheless, a theory of grammar should not be a theory of English and English-like languages alone, and therefore a comprehensive model of indefinite object drop should consider a variety of typologically different languages.\\
As noted by \textcite[134]{Jackendoff2003}, languages such as Korean and Japanese allow for null arguments more easily than English, so that they would pose "no justiﬁcation for distinguishing between obligatorily and optionally expressed semantic arguments". However, in the same paragraph he also argues in favor of the distinction between definite and indefinite null objects being a fully idiosyncratic lexical property of verbs or, at most, of semantic verb classes, a position which I argued against throughout \refch{objectdrop} and \refch{factors}. Thus, it is not to be excluded that languages such as Korean and Japanese may show different degrees of acceptability of indefinite object drop with different optionally transitive verbs, even if they are free in their Topic-drop-based licensing of definite null objects (just like English and typologically similar languages are, as argued on \refpage{recipes}).\\
Another typologically different family, namely, Slavic languages, may shed light on the role played by grammatical aspect in the indefinite implicit object construction. As mentioned in \refsec{perfectivity}, simplifying a very long tradition of studies in a way that will surely disgruntle many of those who fostered research in this area, in Slavic languages perfectivity is embedded in the lexicon rather than being expressed morphologically (as in English and Italian). Should experimental evidence confirm the claim that Slavic languages do not allow for object drop with perfective verbs \parencite{sopata2016null, TsimpliPapadopoulou2006}, this would mean that \textsc{Perf Coda} acts as a hard constraint in Slavic languages, being always re-ranked above \textsc{*Int Arg} for perfective inputs (refer to \refch{medina} and \refch{model}), instead of being a soft, re-rankable constraint as it is in English and Italian. Indeed, as discussed in \refsec{telperftense}, in these two languages telicity, perfectivity and tense are intertwined, so that the interpretation of one factor partially depends on the others. This explains why in my models telicity and (secondarily) perfectivity both play a gradient role in favoring object drop, depending on each verb's semantic selectivity, despite the behavioral experiments being carefully designed to isolate the effect of each factor at play. Based on previous considerations, I hypothesize that a model of indefinite object drop in Slavic languages would paint quite a different picture.


\subsection{Different math}

In this dissertation I followed \textcite{Medina2007} in providing a Stochastic Optimality Theoretic model of indefinite object drop (see \refch{medina}) where the binary predictors described in \refch{predictors} are used to define four faithfulness constraints re-ranking with respect to \textsc{*Int Arg} (a markedness constraint). Semantic selectivity, modeled along a continuous numerical scale, cannot give rise to a binary constraint itself. Instead, Medina implements it in the model by defining the probability of each faithfulness constraint re-ranking with respect to \textsc{*Int Arg} as a (linear) function of the semantic selectivity of the input verb, deviating from the Stochastic Optimality Theoretic norm of associating fixed normal curves to constraints (refer to \refsec{stochot}). However, as \textcite[110]{Medina2007} herself comments, there is no compelling reason why these re-ranking functions should be necessarily \textit{linear} functions. Indeed, she argues that linear functions are "a reasonable place to begin to explore the relative contribution of
Semantic Selectivity to the implicit object construction", and I followed in her steps to obtain models comparable to hers. Future research on indefinite object drop may benefit from employing non-linear functions, defining a more complex algorithm than the one used here to determine the best function and its parameters.\\
Going back to linear Stochastic Optimality Theoretic models of object drop, in \refsec{concl_lmem} I commented on the differences between them and linear mixed-effects models, which are linear regression models including both fixed and random effects in the computation. Mixed models, by their very nature, are able to account both for the effect of the predictors of object drop on the grammaticality of indefinite null objects (the fixed effects), and for the effect of the source of random variability in the data, i.e., the target verbs and the participants to the experiment (the aptly-named random effects). Medina's model and my own, instead, are more like classic linear regression model in that they only account for fixed effects. I minimize any effect the human participants may have had on the results by modeling their normalized ratings (refer to \refsec{likert_preprocessing}), but this pre-processing adaptation of the Likert ratings is more of a quick fix than a solid method I endorse for future research. Indeed, ideally the model should take raw ratings as input, and not only account for the different use the participants made of the Likert scale, but also quantify the amount of variance in the data depending on the participants alone. The same goes for random effects depending on the target verbs, which my models are not able to compute.

\subsection{A follow-up on recoverability and prototypicality}

In \refsec{whichobjects} I argued, with reference to the literature, that indefinite null objects refer to the prototypical Patients of a given transitive verb, as recovered by speakers via world knowledge. However, this prototypicality is not to be intended as a monolith. Rather, it depends on extra- and intra-linguistic context, as it is possible to argue based on \posscite{Rice1988} examples in \ref{rice_concl}. In \ref{rice1_concl}, the act of smoking is intended to refer to cigarettes, because they are the most common object of smoking in contemporary Western society, but different context (such as a mention to olden times, or a Middle-East setting) may induce a reading where the omitted object refers to a pipe or a waterpipe. In \ref{rice2_concl}, the act of drinking is linked to alcohol assumption for reasons made clear in \refch{objectdrop} and \refsec{agentaffect}, but it would necessarily refer to some other prototype were the subject a toddler or a teetotaler, let alone a non-human participant. Similarly, John may be understood to drive a bike in \ref{rice3_concl} and to read a newspaper in \ref{rice4_concl}, provided slight differences in the provided context (e.g., a downtown location to drive to, or a different reading time).

\ex. \label{rice_concl} \a. \label{rice1_concl} John smokes (cigarettes / *Marlboros / *a pipe / *SMOKING MATERIALS).
\b. \label{rice2_concl} John drinks (alcohol / *gin / *water / *coffee / *LIQUIDS).
\c. \label{rice3_concl} When he goes to Boston, John drives (a car / *a Toyota / *a motorcycle / *A VEHICLE).
\d. \label{rice4_concl} Each afternoon, John reads (a book / *Ulysses / *the newspaper / *PRINTED MATTER).

Therefore, possibly enriching some additional models I envisioned in \refsec{expandingthemodel}, it may be useful to design an experiment targeted at the prototypicality intrinsic to object recoverability. I would imagine a cloze-test experiment where subjects have to fill in the gap in sentences such as \textit{John smokes \_\_\_}, manipulating context as to have no-context sentences, common-sense contexts, and uncommon contexts. I hypothesise that there would be much greater agreement between participants relative to the fillers of common-context stimuli and no-context stimuli (where the context is inherently provided via world knowledge), than with uncommon-context stimuli (where it is difficult to imagine prototypicality). I suspect a reaction-time experiment relative to the acceptability of such object-less stimuli would yield results consistent with these expectations.