% \setchapterstyle{kao}
\setchapterpreamble[u]{\margintoc}
\chapter{Exploring the acceptability judgments}
\labch{results}


\section{Making sense of the results: computational implementation} \labsec{likert_scripts}

\subsection{Operative pipeline} \labsec{likert_pipeline}

This paragraph outlines the technical steps needed to obtain the same results I got using the same scripts I coded, for the sake of replicability. I am improving on the operative pipeline employed by \textcite{Medina2007}, which implied collecting judgments on an undisclosed platform and creating a Stochastic Optimality Theoretic model by means of Excel Solver, both because I use non-proprietary software and languages one can download and use for free, and because I am making my Python scripts and raw input data publicly accessible. As pointed out before, all the raw input data necessary to run my scripts, which in this case are the Likert judgments provided by human participants to the experiment in \refch{judgments}, are available \href{https://github.com/giuliacappelli/dissertationData}{in a dedicated GitHub repository}\footnote{https://github.com/giuliacappelli/dissertationData}.\\
Since the script taking care of the data pre-processing, preliminary analysis, and model creation are designed to work on the barest of input data (i.e., lacking any platform-dependent additional information such as datestamps, operative systems, etc.), the first step in this pipeline is the cleansing and reshaping of the output generated in the PsychoPy-Pavlovia-Prolific process of judgment gathering detailed in \refsec{participants}. This result can be achieved using \href{https://github.com/giuliacappelli/PsychopyToMedina}{my dedicated script}\footnote{https://github.com/giuliacappelli/PsychopyToMedina} on GitHub, which takes care of taking the full Pavlovia-generated file as input and yielding a tabular output with the minimal information necessary to run my Stochastic Optimality Theoretic analysis. The script also anonymizes the participants' names to make the data shareable, and it can (optionally, depending on the experimenter's needs) filter out any participant providing polar either-1-or-7 judgments when prompted to make full use of the 7-point Likert scale.\\
The judgments are now ready to be processed with \href{https://github.com/giuliacappelli/MedinaStochasticOptimalityTheory}{the main Python program}\footnote{https://github.com/giuliacappelli/MedinaStochasticOptimalityTheory}, which requires an input comprised of columns for the verb lemmas, the sentence type (target, control or filler), a column for each predictor of object drop in the desired model, and a column with the judgments provided by each participant in the experiment. This script preprocesses the judgments as described in \refsec{likert_preprocessing}, generates the data used in the analysis provided in this Chapter, and models the judgments according to the Stochastic Optimality Theory requirements described in \refch{modeltheory} (final results in \refch{model}).

\subsection{Data preprocessing} \labsec{likert_preprocessing}

Before moving forward to the actual data analysis and modeling, the main script carries out three preprocessing steps:
\begin{enumerate}
    \item computing the min-max normalized semantic selectivity values for Resnik's SPS, Computational PISA, and Behavioral PISA input files (first introduced in \refsec{predictor_sps}), to make the results comparable across models;
    \item multiplying the semantic selectivity score of each verb by its Zipf value (first introduced in \refsec{verbs}), i.e., the base 10 logarithm of the frequency-per-billion-words of the verb in a given corpus, to avoid having the verb's frequency confound the information provided by the semantic selectivity models;
    \item computing the within-subject z-scores for the judgments, then averaging these scores to obtain the mean judgment for each sentence in the stimuli list, then normalizing the mean judgments between 0 and 1 (following the technique by \textcite{KimEtAl2018, KimEtAl2019, KimEtAl2019a}), to account for inevitable differences in the way each participant makes use of the Likert scale.
\end{enumerate}

This kind of preprocessing also improves on \posscite{Medina2007} setting, where both semantic selectivity and judgment data were analysed by considering their raw, original values, because it minimizes the potentially disruptive influence of external factors such as corpus frequencies and individual differences in humans on the final Stochastic Optimality Theoretic model of object drop.\\
In the next sections, following \textcite{Medina2007}, I will provide a thorough description of the way the acceptability judgments pertaining the implicit object construction are influenced by each factor separately and by all the factors together, both in English and in Italian. This analysis will demonstrate that:
\begin{itemize}
    \item no factor alone has a main effect so strong as to fully predict the grammaticality of the implicit object construction;
    \item a comprehensive Stochastic Optimality Theoretic model of object drop based on all five predictors in \refch{predictors} is indeed feasible.
\end{itemize}
The model itself is presented and discussed in \refch{model}.


\section{English results} \labsec{eng_judgresult}

\subsection{Semantic selectivity} \labsec{eng_judgresult_semsel}

The effect of semantic selectivity on the acceptability of the implicit object in English is quantified by means of a Pearson correlation between them. The results of this computation are visualized in \reffig{explore_eng_semsel_sps} for Resnik's SPS, in \reffig{explore_eng_semsel_cpisa} for Computational PISA, and in \reffig{explore_eng_semsel_bpisa} for Behavioral PISA.\\
The first thing to strike the eye of the observer is that the three models of semantic selectivity correlate with varying degrees of accuracy with the human judgments. Unsurprisingly the Selectional Preference Strength, a now-classic measure by \textcite{Resnik1993,Resnik1996}, yields unsatisfactory results which fall quite short of statistical significance. Computational PISA performs much better, with significant (p = 0.038) results, even though the correlation between it and the judgments is very modest (Pearson's r = 0.381). Finally, Behavioral PISA appears to be by far the best-performing model of semantic selectivity, with a Pearson's r of 0.494 against human judgments and a p value of 0.006.\\ 
Keeping in mind what I concluded about the three models of semantic selectivity back in \refsec{evalMySPSs} (see \reftab{sps_correlmatrix_eng} in particular), these results should not come as a surprise. Indeed, being the gold standard of semantic selectivity models due to being based on human judgments, Behavioral PISA is expected to yield the best results among the three models used here. Computational PISA correlated quite well with the Behavioral PISA benchmark, and we can see that it also correlates nicely with the acceptability judgments regarding the implicit object construction. Resnik's SPS, on the contrary, was found to be a poor model of semantic selectivity if compared to Behavioral PISA, and it is also a poor fit if compared to human ratings about object drop. The good performance of Computational PISA and Behavioral PISA against Resnik's SPS can be explained by referring to the way these models were created, since both PISA models are based on pairwise similarity scores between pairs of direct objects for a given verb, while Resnik's SPS is taxonomy-based.\\
All in all, semantic selectivity (especially the PISA models) is not a bad predictor of object drop in English, but it's far from being a reliable one when considered in isolation from all the other possible ones.

% parlare dei singoli puntini rilevanti?


\begin{figure}[htb]
\caption{Correlation between semantic selectivity (Resnik's SPS) and normalized acceptability judgments on object drop in English.}
\labfig{explore_eng_semsel_sps}
    \input figures/ukwac_preliminary_scatterplot_SPS.tex
\end{figure}

\begin{figure}[htb]
\caption{Correlation between semantic selectivity (Computational PISA) and normalized acceptability judgments on object drop in English.}
\labfig{explore_eng_semsel_cpisa}
    \input figures/ukwac_preliminary_scatterplot_Computational_PISA.tex
\end{figure}

\begin{figure}[htb]
\caption{Correlation between semantic selectivity (Behavioral PISA) and normalized acceptability judgments on object drop in English.}
\labfig{explore_eng_semsel_bpisa}
    \input figures/ukwac_preliminary_scatterplot_Behavioral_PISA.tex
\end{figure}


\subsection{Binary predictors} 

\paragraph{Telicity}
The boxplots in \reffig{explore_eng_telicity} illustrate the main effect of telicity on the acceptability judgments on the implicit object construction in English. A Mann-Whitney U test reveals that telic verbs were judged as significantly (p < 0.0001) less grammatical than atelic verbs, consistently with expectations (refer back to \refsec{telicity} and \refsec{predictor_telicity}). In particular, the median rating for telic verbs is 0.501 and the median rating for atelic verbs is 0.906.\\
Despite the statistical significance of the difference between the ratings of telic and atelic verbs, it is not the case that all telic verbs receive ratings below a given threshold and all atelic verbs receive ratings above it. On the contrary, judgments for telic verbs span almost all the way from 0 to 1, and while judgments for atelic verbs have a much tighter distribution (with their interquartile range\sidenote{The interquartile range is the difference between the first quartile and the third quartile, which are the medians of the lower and the upper half of the dataset, respectively. Graphically, it is rendered as the so-called "box" in the boxplot. The other parts of a boxplot are the median (second quartile), cutting the interquartile range, and the so-called "whiskers", i.e., the minimum and maximum values in the dataset. Outliers are shown in these boxplots as little diamonds outside of the boundaries traced by the whiskers.} being fully above the interquartile range for telic verbs), they still overlap in a non-negligible way.
\reffig{explore_eng_telicity} only shows a single outlier among the atelic verbs, corresponding to the atelic, manner unspecified verb \textit{to cut} in the perfective, non-iterative sentence stimulus \textit{Sean had cut} (normalized acceptability rating of 0.286). This may depend on the fact that not only this verb is fairly resistant to object drop despite its atelicity, with all its ratings being within the lower 18 positions among the 72 atelic target stimuli, but this stimulus in particular also has two features which tend to favor the use of overt objects in sentences (i.e., perfectivity and lack of iterativity).

\begin{figure}[htb]
\caption{Effect of telicity on normalized acceptability judgments about object drop in English.}
\labfig{explore_eng_telicity}
    \input figures/ukwac_star_boxplot_telicity.tex
\end{figure}


\paragraph{Perfectivity}
The boxplots in \reffig{explore_eng_perfectivity} illustrate the main effect of perfectivity on the acceptability judgments on the implicit object construction in English. The median rating for imperfective stimuli is 0.854 while the median rating for perfective stimuli is 0.703, and a Mann-Whitney U test shows that these medians are significantly different (p < 0.01). This result is compatible with the hypothesis that the imperfective aspect favors the omission of direct objects and perfective aspect resists it (refer back to \refsec{perfectivity} and \refsec{predictor_perfectivity}).\\
However, the distribution of judgments both for imperfective and for perfective stimuli is very sparse, given that both span almost all the way from 0 to 1, and there is significant overlap between both interquartile ranges. Neither imperfective nor perfective stimuli received outlier ratings.

\begin{figure}[htb]
\caption{Effect of perfectivity on normalized acceptability judgments about object drop in English.}
\labfig{explore_eng_perfectivity}
    \input figures/ukwac_star_boxplot_perfectivity.tex
\end{figure}

\paragraph{Iterativity}
The boxplots in \reffig{explore_eng_iterativity} illustrate the main effect of iterativity on the acceptability judgments on the implicit object construction in English. The median rating for iterative stimuli (0.826) is higher than the median rating for non-iterative stimuli (0.753), consistently with the literature on the matter (refer back to \refsec{iterativity} and \refsec{predictor_iterativity}). However, the difference is not stark enough to be statistically significant according to a Mann-Whitney U test, which may depend on native speakers being less sensitive to iterativity if compared to other linguistic factors (such as telicity and perfectivity) when it comes to judging the grammaticality of the implicit object construction. Another possible explanation for this apparently irrelevant role of iterativity may be that it is only lacking when considered as a main effect, but it may be a relevant predictor when considered in conjunction with several other factors in a joint model. I will get back to this in \refsec{eng_jointeffect}.\\
Once again, the distribution of judgments for both types of stimuli covers almost all the possible 0-1 range, and there are no outlier ratings.

\begin{figure}[htb]
\caption{Effect of iterativity on normalized acceptability judgments about object drop in English.}
\labfig{explore_eng_iterativity}
    \input figures/ukwac_star_boxplot_iterativity.tex
\end{figure}

\paragraph{Manner specification}
The boxplots in \reffig{explore_eng_mannspec} illustrate the main effect of manner specification on the acceptability judgments on the implicit object construction in English. A Mann-Whitney U test shows that the median rating for manner non-specified verbs (0.898) is significantly higher (p < 0.001) than the median rating for manner specified verbs (0.645), consistently with the literature and the hypothesis (refer back to \refsec{mannerspec} and \refsec{predictor_mannspec}).\\
The distribution of judgments for manner specified verbs (0.060 $\div$ 0.987) is more sparse than the distribution of judgments for manner non-specified verbs (0.286 $\div$ 1), if one does not consider the five outliers among the latter. These outliers are the ratings for:
\begin{itemize}
    \item the four target stimuli for the verb \textit{to break} (perfective non-iterative 0, imperfective iterative 0.038, perfective iterative 0.039, imperfective non-iterative 0.050);
    \item the perfective, non-iterative stimulus for the verb \textit{to build} (0.124), i.e., \textit{Paul had built}.
\end{itemize}
As was the case with the atelic outlier in \reffig{explore_eng_telicity}, the outlier stimulus for the verb \textit{to build} is both perfective and non-iterative, making it a very unlikely candidate for felicitous object drops. The verb \textit{to break} appears to be quite resistant to object drop regardless of the experimental conditions, given that all the target stimuli featuring it are outliers (below the lower whisker of the boxplot) in the distribution of judgments for manner non-specified verbs.

\begin{figure}[htb]
\caption{Effect of manner specification on normalized acceptability judgments about object drop in English.}
\labfig{explore_eng_mannspec}
    \input figures/ukwac_star_boxplot_mannspec.tex
\end{figure}


\subsection{Joint effect of predictors}  \labsec{eng_jointeffect}

So far in \refsec{eng_judgresult} I demonstrated with experimental evidence that no predictor among the five I am focusing on (detailed in \refch{factors} and \refch{predictors}) can predict alone the grammaticality of the implicit object construction, regardless of the statistical significance of its main effect on human acceptability judgments. In this section I will instead consider the joint effect of all five predictors of object drop on the grammaticality ratings in a statistical model, in order to gauge the feasibility of a linguistically-motivated probabilistic model of the implicit object construction. I will compute a linear mixed-effects model\sidenote{For observations on the feasibility of linear mixed-effects models applied to rating data, such as Likert-scale judgments, refer to \textcite{GibsonPiantadosiFedorenko2011, bross2019acceptability, cunnings2012overview, Kizach2014, EndresenJanda2017}.} for each measure of semantic selectivity I employed, accounting both for the fixed effects determined by the five predictors and for the random effects determined by my choice of verbs and participants, whereas \textcite[131]{Medina2007} computed a multiple linear regression (which is not endowed to account for random effects in addition to the fixed ones). I carried out my analysis using the Python package \texttt{statsmodels} and completed it with the R function \texttt{report()} of the package \texttt{easystats} \parencite{r_report}, which takes as its input the linear mixed model created with the R function \texttt{lmer()} of the package \texttt{lme4} \parencite{r_lmer}, to compute the conditional R\textsuperscript{2} and marginal R\textsuperscript{2} of the model\sidenote{The conditional R\textsuperscript{2} quantifies the total explanatory power of the model, i.e., how much both fixed effects and random effects explain the variance in the data. The marginal R\textsuperscript{2} instead quantifies the explanatory power of fixed effects alone.}.\\
The three linear mixed-effects models I computed are reported in:
\begin{itemize}
    \item \reftab{eng_lmem_sps} (conditional R\textsuperscript{2} = 0.53, marginal R\textsuperscript{2} = 0.20), where semantic selectivity is measured with Resnik's SPS;
    \item \reftab{eng_lmem_cpisa} (conditional R\textsuperscript{2} = 0.53, marginal R\textsuperscript{2} = 0.20), where semantic selectivity is measured with Computational PISA;
    \item \reftab{eng_lmem_bpisa} (conditional R\textsuperscript{2} = 0.53, marginal R\textsuperscript{2} = 0.22), where semantic selectivity is measured with Behavioral PISA.
\end{itemize}

In general, it is already possible to observe that, while the total explanatory power is the same for the three models, Behavioral PISA generates a slightly better model than Resnik's SPS and Computational PISA when considering the fixed effects alone.

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Linear mixed-effects model of the five predictors of object drop in English as fixed effects, with verb and participant subject as random effects, measuring semantic selectivity with Resnik's SPS.}
\labtab{eng_lmem_sps}
\begin{tabular}{l|llllll}
                         & Coef. & Std.Err. & z      & P\textgreater{}|z| & {[}0.025 & 0.975{]} \\
\hline                         
Intercept                & 4.117 & 0.334    & 12.333 & 0.000                  & 3.463    & 4.771    \\
telicity{[}atelic{]}     & 1.504 & 0.388    & 3.870   & 0.000                  & 0.742    & 2.265    \\
perfectivity{[}imperf{]} & 0.541 & 0.045    & 11.903 & 0.000                  & 0.452    & 0.630     \\
iterativity{[}iter{]}    & 0.221 & 0.045    & 4.864  & 0.000                  & 0.132    & 0.310     \\
mannspec{[}nospec{]}     & 0.280  & 0.383    & 0.732  & 0.464              & -0.470    & 1.031    \\
sps                      & 0.199 & 0.183    & 1.088  & 0.277              & -0.159   & 0.557    \\
subject                 Var   & 0.351    & 0.071  &                    &          &       &     \\
verb                 Var   & 0.943    & 0.196  &                    &          &         &     
\end{tabular}
\end{table}

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Linear mixed-effects model of the five predictors of object drop in English as fixed effects, with verb and participant subject as random effects, measuring semantic selectivity with Computational PISA.}
\labtab{eng_lmem_cpisa}
\begin{tabular}{l|llllll}
                         & Coef. & Std.Err. & z      & P\textgreater{}|z| & {[}0.025 & 0.975{]} \\
\hline                         
Intercept                & 4.167 & 0.352    & 11.854 & 0.000              & 3.478    & 4.856    \\
telicity{[}atelic{]}     & 1.437 & 0.407    & 3.526  & 0.000              & 0.638    & 2.235    \\
perfectivity{[}imperf{]} & 0.541 & 0.045    & 11.903 & 0.000              & 0.452    & 0.630    \\
iterativity{[}iter{]}    & 0.221 & 0.045    & 4.864  & 0.000              & 0.132    & 0.310    \\
mannspec{[}nospec{]}     & 0.261 & 0.390    & 0.669  & 0.504              & -0.504   & 1.026    \\
cpisa                    & 0.182 & 0.198    & 0.920  & 0.358              & -0.206   & 0.571    \\
subject                 Var   & 0.351    & 0.071  &                    &          &    &        \\
verb                     Var   & 0.956    & 0.198  &                    &          &   &      
\end{tabular}  
\end{table}

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Linear mixed-effects model of the five predictors of object drop in English as fixed effects, with verb and participant subject as random effects, measuring semantic selectivity with Behavioral PISA.}
\labtab{eng_lmem_bpisa}
\begin{tabular}{l|llllll}
                         & Coef. & Std.Err. & z      & P\textgreater{}|z| & {[}0.025 & 0.975{]} \\
\hline                         
Intercept                & 4.238 & 0.336    & 12.601 & 0.000              & 3.579    & 4.898    \\
telicity{[}atelic{]}     & 1.278 & 0.405    & 3.157  & 0.002              & 0.485    & 2.072    \\
perfectivity{[}imperf{]} & 0.541 & 0.045    & 11.903 & 0.000              & 0.452    & 0.630    \\
iterativity{[}iter{]}    & 0.221 & 0.045    & 4.864  & 0.000              & 0.132    & 0.310    \\
mannspec{[}nospec{]}     & 0.306 & 0.367    & 0.832  & 0.406              & -0.415   & 1.026    \\
bpisa                    & 0.331 & 0.190    & 1.740  & 0.082              & -0.042   & 0.705    \\
subject                  Var   & 0.351    & 0.071  &                    &          &    &      \\
verb                     Var   & 0.883    & 0.183  &                    &          &     &       
\end{tabular}
\end{table}

The three models all converge\sidenote{Linear mixed-effects model are generated by optimizing a complex function over thousands of steps. The optimizer, which in the case of lme4::lmer is nloptwrap by default, stops as soon as it finds a solution to the optimization problem or after a given number of unsuccessful iterations. In the first case, the model is said to have converged and it is reliable. In the other case, a warning is issued that the model has not converged, meaning that the estimates it yielded may not be reliable.} on similar results. In particular, they show that:
\begin{itemize}
    \item the effect of (im)perfectivity, (a)telicity, and iterativity is statistically significant and positive;
    \item the effect of manner (non-)specification and semantic selectivity is statistically non-significant and positive.
\end{itemize}
This leads to the conclusion that the joint effect of the five predictors of object drop in English is by far a better explanation of the acceptability judgments on this phenomenon than the isolated main effects of them all. This means that it will make sense to compute a linguistically-motivated, stochastic (Optimality Theoretic) model of the effect of all the five predictors on the grammaticality of the implicit object construction.


\section{Italian results} \labsec{ita_judgresult}

\subsection{Semantic selectivity} \labsec{ita_judgresult_semsel}

The effect of semantic selectivity on the acceptability of the implicit object in Italian is quantified by means of a Pearson correlation between them. The results of this computation are visualized in \reffig{explore_ita_semsel_sps} for Resnik's SPS, in \reffig{explore_ita_semsel_cpisa} for Computational PISA, and in \reffig{explore_ita_semsel_bpisa} for Behavioral PISA.\\
What I observed in \refsec{eng_judgresult} about the correlations between the three models of semantic selectivity and human judgments about object drop in English still holds true, \textit{mutatis mutandis}, when considering the Italian data. First of all, it appears that Resnik's SPS is once again the worst-performing model among the three (with a staggeringly low, non-significant Pearson's r of -0.055), Computational PISA makes the situation somewhat better but still not enough to be statistically significant (Pearson's r = 0.223), and Behavioral PISA is quite a good model of semantic selectivity (Pearson's r = 0.481, p value = 0.007).\\
Once again, this state of affairs mirrors the situation depicted in \refsec{evalMySPSs} (see \reftab{sps_correlmatrix_ita} in particular), where I made the case that Behavioral PISA, the human judgment-based benchmark model of semantic selectivity, correlates better with Computational PISA than with Resnik's SPS. Moreover, the non-significant correlation yielded by both Resnik's SPS and Computational PISA in Italian relative to the acceptability judgments on the implicit object construction mirrors the high correlation shown in \reftab{sps_correlmatrix_ita} between Resnik's SPS and Computational PISA. It would thus appear that the itWaC corpus has a stronger effect on the semantic similarity measures based on it than ukWaC has on the ones computed for English, as shown earlier in \refsec{evalMySPSs}.\\
Concluding, Behavioral PISA is a satisfactory predictor of object drop in Italian, with a correlation against human acceptability judgments on object drop comparable with the one obtained by Behavioral PISA in English (compare \reffig{explore_eng_semsel_bpisa} and \reffig{explore_ita_semsel_bpisa}). However, as is the case with English, Behavioral PISA is not able to fully predict the feasibility of object drop for a given transitive verb.

% % parlare dei singoli puntini rilevanti?

\begin{figure}[htb]
\caption{Correlation between semantic selectivity (Resnik's SPS) and normalized acceptability judgments on object drop in Italian.}
\labfig{explore_ita_semsel_sps}
    \input figures/itwac_preliminary_scatterplot_SPS.tex
\end{figure}

\begin{figure}[htb]
\caption{Correlation between semantic selectivity (Computational PISA) and normalized acceptability judgments on object drop in Italian.}
\labfig{explore_ita_semsel_cpisa}
    \input figures/itwac_preliminary_scatterplot_Computational_PISA.tex
\end{figure}

\begin{figure}[htb]
\caption{Correlation between semantic selectivity (Behavioral PISA) and normalized acceptability judgments on object drop in Italian.}
\labfig{explore_ita_semsel_bpisa}
    \input figures/itwac_preliminary_scatterplot_Behavioral_PISA.tex
\end{figure}


\subsection{Binary predictors} 

\paragraph{Telicity}
The boxplots in \reffig{explore_ita_telicity} illustrate the main effect of telicity on the acceptability judgments on the implicit object construction in Italian. A Mann-Whitney U test reveals that the median judgment for atelic verbs (0.823) is significantly higher (p < 0.0001) than the median judgment for telic verbs (0.384), consistently with abundant literature on the effect of telicity on object drop (refer back to \refsec{telicity} and \refsec{predictor_telicity}).\\
The interquartile ranges of telic and atelic verbs do not overlap, as shown in the boxplots, but the overall distributions of ratings for the two types of verbs do indeed overlap for the most part. This shows that, despite the high statistical significance of the difference in judgments between telic and atelic verbs, telicity alone is not a sufficient predictor of object drop in Italian.

\begin{figure}[htb]
\caption{Effect of telicity on normalized acceptability judgments about object drop in Italian.}
\labfig{explore_ita_telicity}
    \input figures/itwac_star_boxplot_telicity.tex
\end{figure}

\paragraph{Perfectivity}
The boxplots in \reffig{explore_ita_perfectivity} illustrate the main effect of perfectivity on the acceptability judgments on the implicit object construction in Italian. The median rating for imperfective stimuli (0.670) is significantly higher (p < 0.05) than the median rating for perfective stimuli (0.562), consistently with the hypothesis (refer back to \refsec{perfectivity} and \refsec{predictor_perfectivity}).\\
However, the distribution of ratings for both imperfective and perfective stimuli is quite sparse, and thus there is significant overlap between them. The significant main effect of perfectivity on the grammaticality of the implicit object construction cannot be considered reason enough to use it as the sole predictor of object drop.

\begin{figure}[htb]
\caption{Effect of perfectivity on normalized acceptability judgments about object drop in Italian.}
\labfig{explore_ita_perfectivity}
    \input figures/itwac_star_boxplot_perfectivity.tex
\end{figure}

\paragraph{Iterativity}
The boxplots in \reffig{explore_ita_iterativity} illustrate the main effect of iterativity on the acceptability judgments on the implicit object construction in Italian. Interestingly, both in English (see \refsec{eng_judgresult}) and Italian there is no significant main effect of iterativity on the grammaticality of the implicit object construction, and once again the question arises of whether this depends on the weakness of this factor if compared against the other predictors, or whether it will be solved by considering its action in a joint statistical model of all five predictors. The median for iterative stimuli is indeed higher than the median for non-iterative stimuli (0.659 the former, 0.645 the latter), consistently with literature on the matter (refer back to \refsec{iterativity} and \refsec{predictor_iterativity}), but the difference is way too small to even approach statistical significance. Moreover, there is almost complete overlap between the distributions of judgments for both types of stimuli.

\begin{figure}[htb]
\caption{Effect of iterativity on normalized acceptability judgments about object drop in Italian.}
\labfig{explore_ita_iterativity}
    \input figures/itwac_star_boxplot_iterativity.tex
\end{figure}

\paragraph{Manner specification}
The boxplots in \reffig{explore_ita_mannspec} illustrate the main effect of manner specification on the acceptability judgments on the implicit object construction in Italian. A Mann-Whitney U test reveals the difference between the medians of judgments for manner specified (0.469) and manner non-specified (0.833) verbs to be statistically significant (p < 0.0001), consistently with expectations (refer back to \refsec{mannerspec} and \refsec{predictor_mannspec}).\\
The distribution of ratings for manner non-specified verbs is tighter than the distribution of ratings for manner specified verbs, but there is still relevant overlap between them despite the high statistical significance of their difference. Thus manner specification, just like all the other predictors considered here, cannot fully predict the grammaticality of object drop in isolation from other linguistic factors.

\begin{figure}[htb]
\caption{Effect of manner specification on normalized acceptability judgments about object drop in Italian.}
\labfig{explore_ita_mannspec}
    \input figures/itwac_star_boxplot_mannspec.tex
\end{figure}


\subsection{Joint effect of predictors} \labsec{ita_jointeffect}

Mirroring what I did in \refsec{eng_jointeffect} about English, I will now report the results of three linear-mixed effects models (one for each different measure of semantic selectivity) I computed to account for the joint effect of my five predictors of object drop on the acceptability ratings provided by native speakers of Italian. These models are reported in:
\begin{itemize}
    \item \reftab{ita_lmem_sps} (conditional R\textsuperscript{2} = 0.47, marginal R\textsuperscript{2} = 0.14), where semantic selectivity is measured with Resnik's SPS;
    \item \reftab{ita_lmem_cpisa} (conditional R\textsuperscript{2} = 0.47, marginal R\textsuperscript{2} = 0.14), where semantic selectivity is measured with Computational PISA;
    \item \reftab{ita_lmem_bpisa} (conditional R\textsuperscript{2} = 0.47, marginal R\textsuperscript{2} = 0.15), where semantic selectivity is measured with Behavioral PISA.
\end{itemize}

It appears that, in Italian as in English, the three models have the same total explanatory power (as quantified by the conditional R\textsuperscript{2}), but Behavioral PISA is the best measure of semantic selectivity if compared with Resnik's SPS and Computational PISA because it contributes to determine, \textit{ceteris paribus}, the best linear mixed-effects model when only considering the fixed effects.

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Linear mixed-effects model of the five predictors of object drop in Italian as fixed effects, with verb and participant subject as random effects, measuring semantic selectivity with Resnik's SPS.}
\labtab{ita_lmem_sps}
\begin{tabular}{l|llllll}
                         & Coef. & Std.Err. & z      & P\textgreater{}|z| & {[}0.025 & 0.975{]} \\
\hline                         
Intercept                & 4.677  & 0.255    & 18.367 & 0.000              & 4.178    & 5.176    \\
telicity{[}atelic{]}     & 0.950  & 0.279    & 3.402  & 0.001              & 0.403    & 1.498    \\
perfectivity{[}imperf{]} & 0.308  & 0.041    & 7.610  & 0.000              & 0.229    & 0.388    \\
iterativity{[}iter{]}    & 0.061  & 0.041    & 1.495  & 0.135              & -0.019   & 0.140    \\
mannspec{[}nospec{]}     & 0.550  & 0.279    & 1.976  & 0.048              & 0.004    & 1.096    \\
sps                      & -0.093 & 0.131    & -0.709 & 0.478              & -0.351   & 0.164    \\
subject Var              & 0.403  & 0.090     &        &                    &          &          \\
verb Var                 & 0.489  & 0.115    &        &                    &          &         
\end{tabular}
\end{table}

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Linear mixed-effects model of the five predictors of object drop in Italian as fixed effects, with verb and participant subject as random effects, measuring semantic selectivity with Computational PISA.}
\labtab{ita_lmem_cpisa}
\begin{tabular}{l|llllll}
                         & Coef. & Std.Err. & z      & P\textgreater{}|z| & {[}0.025 & 0.975{]} \\
\hline                         
Intercept                & 4.674  & 0.266    & 17.550 & 0.000              & 4.152    & 5.196    \\
telicity{[}atelic{]}     & 0.970  & 0.288    & 3.369  & 0.001              & 0.406    & 1.535    \\
perfectivity{[}imperf{]} & 0.308  & 0.041    & 7.610  & 0.000              & 0.229    & 0.388    \\
iterativity{[}iter{]}    & 0.061  & 0.041    & 1.495  & 0.135              & -0.019   & 0.140    \\
mannspec{[}nospec{]}     & 0.533  & 0.286    & 1.863  & 0.062              & -0.028   & 1.094    \\
cpisa                    & -0.033 & 0.141    & -0.237 & 0.813              & -0.310   & 0.243    \\
subject Var              & 0.403  & 0.090     &        &                    &          &          \\
verb Var                 & 0.498  & 0.117    &        &                    &          &         
\end{tabular}  
\end{table}

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Linear mixed-effects model of the five predictors of object drop in Italian as fixed effects, with verb and participant subject as random effects, measuring semantic selectivity with Behavioral PISA.}
\labtab{ita_lmem_bpisa}
\begin{tabular}{l|llllll}
                         & Coef. & Std.Err. & z      & P\textgreater{}|z| & {[}0.025 & 0.975{]} \\
\hline                         
Intercept                & 4.800 & 0.280    & 17.150 & 0.000              & 4.252    & 5.349    \\
telicity{[}atelic{]}     & 0.830 & 0.311    & 2.668  & 0.008              & 0.220    & 1.439    \\
perfectivity{[}imperf{]} & 0.308 & 0.041    & 7.610  & 0.000              & 0.229    & 0.388    \\
iterativity{[}iter{]}    & 0.061 & 0.041    & 1.495  & 0.135              & -0.019   & 0.140    \\
mannspec{[}nospec{]}     & 0.454 & 0.281    & 1.614  & 0.106              & -0.097   & 1.005    \\
bpisa                    & 0.140 & 0.155    & 0.903  & 0.367              & -0.164   & 0.443    \\
subject Var              & 0.403 & 0.090    &        &                    &          &          \\
verb Var                 & 0.483 & 0.114    &        &                    &          &         
\end{tabular}
\end{table}

The three models all converge, and for the most part they yield comparable results. In more detail, they show that:
\begin{itemize}
    \item the effect of (a)telicity and (im)perfectivity is statistically significant and positive;
    \item the effect of manner (non-)specification is positive, but only statistically significant in the model quantifying semantic selectivity with Resnik's SPS;
    \item the effect of iterativity is statistically non-significant and positive;
    \item the effect of semantic selectivity, which is never statistically significant, is slightly negative in the models using Resnik's SPS and Computational PISA, but positive in the model using Behavioral PISA (compatibly with everything I observed in this Chapter about the three different models of semantic selectivity in Italian).
\end{itemize}
As noted before about English, these models of the joint effect of the five predictors of object drop on the acceptability judgments in Italian support the creation of a Stochastic Optimality Theoretic model of the implicit object construction, despite the different (sometimes absent) statistical significance of the single linguistic factors.



\section{Closing remarks} \labsec{sumup_judgresult}

The take-home message of this chapter is that no predictor alone, among the five ones I am using in this dissertation, is decisive in determining the grammaticality of the implicit object construction in English or Italian. Nevertheless, all of them (with the exception of iterativity) have statistical significant main effects on the acceptability ratings, so it is evident they play a role in influencing them to some extent.\\
At this point, does it make sense to compute a model of object drop considering the joint effect of all five predictors? I answered positively to this question by means of linear mixed-effects models for English and Italian, which all converged on significant results. These results also point out that the models for English are consistently, albeit just slightly, better than the models for Italian computed with the same set of predictors. This may depend on the corpora of choice (ukWaC for English, itWaC for Italian), on the way the participants to the experiments behaved when providing judgments (despite the strict protocol in \refsec{setting}), and also on idiosyncratic characteristics of the two languages under scrutiny. I will also engage in similar considerations in \refch{model}.\\
The convergence of the linear mixed-effect models proves that a model of object drop considering the joint effect of all five predictors is indeed possible. However, since I just demonstrated and quantified their action, numbers and figures in hand, would it not make sense to be happy with these results and avoid all the hassles that come with having to compute a Stochastic Optimality Theoretic model (in the novel linear, non-gaussian way introduced in \refch{medina}) from scratch? The best answer to this question was already provided by \textcite{Medina2007} herself, the ideator of the original model. This thesis (and hers) concerns itself with creating a \textit{linguistic} model of the grammaticality of the implicit object construction, with explicit constraints whose violations determine which output will be the favored one among a set of possible candidates for a given input. A linear mixed-effects model provides a \textit{statistical} model that computes the relative grammaticality of a candidate as a sum of weighted variables, whereas the Stochastic Optimality Theoretic model by Medina computes it as the sum of the probabilities of constraint orderings. Most importantly, Medina's mathematical model keeps the input, the candidate set, the constraints, and the probabilities of constraint re-ordering explicit and knowable in every step of the computation, while in the mixed model they are all collapsed together in the weights the model computes under the hood.

% confronto italiano-inglese

% dire che l'italiano SPS Ã¨ consistentemente peggio dell'inglese (v. anche Model comparison SPS)

% dire che nel capitolo successivo poi rifletto anche sul rapporto tra regressione e stOT (v. medina p 147)