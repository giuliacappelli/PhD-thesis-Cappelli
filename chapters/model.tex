% \setchapterstyle{kao}
\setchapterpreamble[u]{\margintoc}
\chapter{Predicting the grammaticality of implicit objects}
\labch{model}

\section{Introduction} \labsec{introfitting}

In this Chapter I will model the grammaticality of the implicit object construction (\refch{indefinitedrop}) in a Stochastic Optimality Theoretic fashion inspired by \textcite{Medina2007} (\refch{modeltheory}), using five aspectual and semantic factors (\refch{factors} and \refch{predictors}) as constraints in several models of human acceptability judgments (\refch{judgments}) in English and Italian. Based on the results of these two behavioral experiments described in \refch{results}, a linguistically-motivated probabilistic model of object drop considering the joint effect of all five predictors is indeed feasible.\\
In particular, I will describe the models in general in \refsec{intro_models}, I will delve into the finer details of the full English and Italian models of object drop as a function of Behavioral PISA (introduced in \refsec{behavPisa}) in \refsec{stot_full}, and I will draw some conclusions about relevant linguistic and mathematical aspects of these models in \refsec{stot_conclusions}.

\subsection{Progressive models} \labsec{intro_models}

In this thesis, I build upon the foundations laid by \textcite{Medina2007}, which I detailed in \refsec{medinamodel}. In a nutshell, her Stochastic Optimality Theoretic analysis of the implicit object construction was focused on English, and used a set of only three predictors (semantic selectivity, telicity, and perfectivity) as constraints in the model. Moreover, she measured the verbs' semantic selectivity using the Selectional Preference Strength values originally computed by \textcite{Resnik1993,Resnik1996}, which poses evident limitations in the choice of transitive verbs to include in the model and which also suffers from some computational drawbacks due to being a taxonomy-based measure (more on this in \refsec{evalMySPSs}).\\
Expanding on Medina's successful model of object drop, I bring several new ideas to the table:
\begin{itemize}
    \item quantifying semantic selectivity with two similarity-based measures, i.e. a novel computational one I contributed to develop in \textcite{CappelliLenciPISA} (Computational PISA, see \refsec{compuPisa}), and a behavioral one that improves on Medina's measure of Object Similarity (Behavioral PISA, see \refsec{behavPisa});
    \item modeling the implicit object construction both in English and in Italian, comparing the performance of the two models and possible language-dependent differences in the constraint re-ranking;
    \item computing increasingly more complex Stochastic Optimality Theoretic models of object drop, starting with Medina's three-predictor one, adding iterativity as a predictor in an intermediate model, and computing the full five-predictor model also including manner specification among the predictors.
\end{itemize}
These additions to Medina's setting resulted in a grand total of 18 models of the implicit object construction, which are summarized in \reftab{tab_mymodels} for the reader's convenience.

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{The 18 Stochastic Optimality Theoretic models of object drop I computed for English and Italian.}
\labtab{tab_mymodels}
\begin{tabular}{l|ccc}
& SPS & Comp PISA & Behav PISA \\
\hline
StOT basic & eng | ita          & eng | ita   & eng | ita   \\
StOT +iter & eng | ita  & eng | ita & eng | ita \\
StOT +iter +spec & eng | ita   & eng | ita   & eng | ita  
\end{tabular}
\end{table}

The basic Stochastic Optimality Theoretic model of English judgments using Resnik's SPS as a measure of semantic selectivity is, as recalled earlier in this Section, a reproduction of the model by \textcite{Medina2007} employing the same constraints and acceptability judgments based on the same experimental protocol (but with different target verbs and an updated computational preprocessing pipeline, as explained in \refch{judgments} and \refch{results}). The other 17 models are instead first-time analyses.\\
Naturally, one could ask why it is iterativity, and not manner specification, the predictor of object drop to be included in the intermediate Stochastic Optimality Theoretic models. After all, the main effect of iterativity was shown to be non-significant in \reffig{explore_eng_iterativity} and \reffig{explore_ita_iterativity} both for English and Italian, unlike the very significant main effect of manner specification in both languages (refer back to \reffig{explore_eng_mannspec} and \reffig{explore_ita_mannspec}). Crucially though, I am creating probabilistic models considering the \textit{joint} effect of five linguistic factors on the grammaticality of the implicit object construction. Since the linear mixed-effects models for English (see \refsec{eng_jointeffect}) revealed a highly significant effect of iterativity and a non-significant effect of manner specification, while the two predictors were almost equally non-significant in the mixed models for Italian (see \refsec{ita_jointeffect}), it appeared that iterativity plays a larger role in determining the grammaticality of object drop when considered in combination with all the other linguistic factors involved.


\subsection{The input, the output, and the constraints} \labsec{intro_constraints}

A very short summary of the lengthy explanation of (Stochastic) Optimality Theory I provided in \refch{modeltheory}, and especially of the explanation of the novel variant by \textcite{Medina2007} in \refsec{medinamodel}, is in order. In particular, I am going to retrace the way the input to the optimization process maps to the output, and I will introduce my two novel constraints after looking back on Medina's original set.\\
As shown in \ref{medina_input_generic} within \refsec{inputmedina}, the input to the syntactic optimization operated by the model has to include all the relevant lexical and semantic information that will be mapped to syntactically well-formed output forms, and nothing else. Thus, the input to my basic Stochastic Optimality Theoretic models (and Medina's) will look like \ref{my_input_basic}, the input to the intermediate models will look like \ref{my_input_ext1}, and the input to the full models with all five predictors will look like \ref{my_input_ext2}. All inputs in \ref{my_input_generic} contain a transitive verb with a subject and an unspecified direct object (since the model deals with indefinite, not definite, object drop), a numerical value for semantic selectivity (be it Resnik's SPS, Computational PISA, or Behavioral PISA), the [+Past] feature since all verbs in the stimuli are in the past tense, and the features of the predicate relative to all the binary predictors that are relevant in the model (two in the basic model, three in the intermediate model, four in the full model).

\ex. \label{my_input_generic} 
\a. \label{my_input_basic} verb (x,y), x = subject, y = unspecified, semantic selectivity = \textit{numerical value}, [+ Past], [$\pm$ Telic], [$\pm$ Perfective]
\b. \label{my_input_ext1} verb (x,y), x = subject, y = unspecified, semantic selectivity = \textit{numerical value}, [+ Past], [$\pm$ Telic], [$\pm$ Perfective], [$\pm$ Iterative]
\c. \label{my_input_ext2} verb (x,y), x = subject, y = unspecified, semantic selectivity = \textit{numerical value}, [+ Past], [$\pm$ Telic], [$\pm$ Perfective], [$\pm$ Iterative], [$\pm$ Manner Specified]

Given these inputs, the \textsc{Gen} component of the Optimality Theoretic grammar (see \refsec{classicot}) generates two outputs, i.e. one with an overt (unspecified) direct object and one with an implicit direct object.\\
Now that the grammar yielded a complete candidate set to evaluate, the model has to pick a winner (or, in our case, assign gradient grammaticality to the implicit object output in a probability space) based on the re-ranking of the relevant constraints. For the basic model, these are the ones in \ref{my_constraints_medina} (adapted from Medina's ones in \ref{medina_constraints}, introduced in \refsec{constraintsmedina}).

\ex. \label{my_constraints_medina} 
\a. \label{my_constraints_intarg} \textsc{*Int Arg (*Internal Argument Structure)}\\ The output must not contain an overt direct object.
\b. \label{my_constraints_faith} \textsc{Faith Arg (Faithfulness to Argument Structure)}\\ All arguments in the input must be present in the output.
\c. \label{my_constraints_telic} \textsc{Telic End (Telic Endpoint)}\\ Telic predicates must be bounded by an object in the output.
\c. \label{my_constraints_perf} \textsc{Perf Coda (Perfective Coda)}\\ Perfective predicates must have a direct object in the output.

I also designed the two novel constraints in \ref{my_constraints_new}, based on theoretical observations on iterativity and manner specification first introduced in \refch{factors} and explored further in \refch{predictors}. \textsc{Non-Iter Arg} is active both in the intermediate and in the full model, while \textsc{Mann-Spec Arg} is only active in the full model.

\ex. \label{my_constraints_new} 
\a. \label{my_constraints_iter} \textsc{Non-Iter Arg (Non-Iterative Argument)}\\ Non-iterative predicates must occur with a direct object in the output.
\b. \label{my_constraints_spec} \textsc{Mann-Spec Arg (Manner-Specified Argument)}\\ Manner-specified verbs must occur with a direct object in the output.

In all my Stochastic Optimality Theoretic models, \textsc{*Int Arg} is a markedness constraint that gets violated when there is an overt direct object in the output, directly conflicting with all the other constraints, which are \textit{faithfulness} constraints penalizing implicit objects. This conflict between markedness and faithfulness constraints is the very core of an Optimality Theoretic grammar (refer back to \refch{modeltheory}).\\
In the specific case of this thesis, an implicit object output will be (probabilistically) grammatical whenever \textsc{*Int Arg} is ranked above all the other constraints that are active (i.e. not vacuously satisfied\sidenote{As explained in detail in \refsec{constraintsmedina}, a constraint is vacuously satisfied when no candidate in the candidate set can violate it.}) for a given input. For instance, the full, five-predictor model would only favor object-dropping telic, perfective, non-iterative, manner-specified candidates if \textsc{*Int Arg} was ranked above \textsc{Faith Arg}, \textsc{Telic End}, \textsc{Perf Coda}, \textsc{Non-Iter Arg}, and \textsc{Mann-Spec Arg}. The same model would only require \textsc{*Int Arg} to be ranked above \textsc{Faith Arg} to allow for object drop in atelic, imperfective, iterative, manner non-specified candidates.

% non-iter arg è formulato in modo "negativo" perché altrimenti sarebbe un markedness constraint (è vero? cambierebbe qualcosa?)


\subsection{Model evaluation} \labsec{intro_evaluation}

Let us put aside the inner workings of Medina-inspired Stochastic Optimality Theoretic models for the time being, and let us consider whether increasing the number of predictors (and thus the number of constraints) actually determines a better understanding of the nature of the implicit object construction. I am going to come back to the mathematical details of the model in \refsec{stot_full}, where I will focus on the two best-performing models, one for English and one for Italian. Unfortunately, it would be impossible to provide a complete account of all 18 models in \reftab{tab_mymodels} due to space constraints, but the interested reader can find graphical summaries of their results in \refapp{app_models}.

\paragraph{English} An initial step to assess the absolute performance of each model, and hence to compare them and gauge their performance relative to each other, would be to compute Pearson correlations\sidenote{The Pearson's $\rho$ coefficient measuring the correlation between two variables can vary between -1 and 1. The strength of the correlation is judged by considering the absolute value of the coefficient, so that it is non-existent when $\rho$ is 0, very strong when it is closer to (-)1. If $\rho$ is positive the two variables are directly proportional one to another, while if it is negative they are indirectly proportional.} between the actual acceptability judgments provided by native speakers and the predicted grammaticality values yielded by each model. The Pearson's $\rho$ coefficients for English, all highly significant (p < 0.001), are collected in \reftab{tab_pearson_eng}. These results show that the predicted values correlate quite well with the human-generated values in each model, going from 0.661 for the basic model using Resnik's SPS as a measure of semantic selectivity to 0.700 for the full model using Behavioral PISA as a measure of semantic selectivity.

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Pearson correlations between actual and predicted values for the nine Stochastic OT models of object drop in English.}
\labtab{tab_pearson_eng}
\begin{tabular}{l|ccc}
& SPS & Comp PISA & Behav PISA \\
\hline
StOT basic           & 0.661        & 0.686     & 0.693      \\
StOT +iter           & 0.664        & 0.689     & 0.696      \\
StOT +iter +spec     & 0.670        & 0.691     & 0.700     
\end{tabular}
\end{table}

However, the correlation coefficient only serves to quantify the strength of the linear relationship between actual and predicted judgments, without providing any information on how well the independent variables in the model (i.e. the predictors) explain the variance in the dependent variable (i.e. the acceptability judgments). I gleaned this information by computing the adjusted R\textsuperscript{2} value for each model, obtaining the results in \reftab{tab_adjrsq_eng} relative to English.\\
The R-squared value, also known as "coefficient of determination", can be computed as the squared Pearson's $\rho$ or, alternatively, as in \refeq{rsquared} (with the Summed Squared Error computed as in \refeq{sse} and the Total Sum of Squares computed as in \refeq{tss}).

\begin{equation} \labeq{rsquared}
R^2 = 1 - \frac{\textrm{Summed Squared Error}}{\textrm{Total Sum of Squares}}
\end{equation}

The Summed Squared Error, computed with \refeq{sse}, is defined as the sum of the squared difference between each actual judgment and its corresponding judgment predicted by the model, for all judgments in the sample.

\begin{equation} \labeq{sse}
SSE = \sum_{i=1}^{n} (y_i - \widehat{y})^2
\end{equation}

The Total Sum of Squares is defined as the sum of the squared difference between each acceptability judgment in the sample and the average acceptability judgment, for all judgments in the sample. This is shown in \refeq{tss}.

\begin{equation} \labeq{tss}
TSS = \sum_{i=1}^{n} (y_i - \overline{y})^2
\end{equation}

R\textsuperscript{2} varies between 0 and 1, and it can be thought of as a percentage indicating the goodness of fit of a statistical model. However, it always increases when using additional predictors in a model, regardless of the usefulness of these variables in predicting the dependent variable. One could always add yet one more parameter to the model, overfit it to the data, and claim to have a very successful model due to a very high R\textsuperscript{2} value. In order to overcome this major drawback of R\textsuperscript{2}, it is recommended to compute an \textit{adjusted} R\textsuperscript{2} value that only increases when adding relevant parameters, while it decreases when adding useless ones. It is defined as in \refeq{adjR2}, where $n$ is the number of acceptability judgments to be predicted and $k$ is the number of independent variables (i.e. predictors) in the model, and it can be thought of as the percentage of variance explained by the sole indipendent variables that have an actual effect on the dependent variable.

\begin{equation} \labeq{adjR2}
\textrm{adjusted } R^2 = 1 - \frac{(1-R^2)(n-1)}{n-k-1}
\end{equation}

Let us close this much needed statistical parenthesis and go back to the summary of the English results. Oddly enough, \textcite[147]{Medina2007} limited her model assessment to the computation of the Summed Squared Error instead of also using it to compute the (adjusted) R\textsuperscript{2} value, which makes it impossible to compare mathematically her results and the ones I obtained in the basic model using Resnik's SPS as a measure of semantic selectivity. Moreover, the Summed Squared Error does not have an intrinsic meaning (unlike R\textsuperscript{2}), since it just increases whenever the total number of stimuli in the experiment increases (provided there is some difference between the actual and predicted values).\\
According to the adjusted R\textsuperscript{2} values in \reftab{tab_adjrsq_eng}, the nine models explain between 42.1\% (intermediate model with Resnik's SPS) and 46.8\% (full model with Behavioral PISA) of the variation in the data. Given the complex nature of the implicit object construction and the interaction between all the predictors, these results, though modest in absolute terms, are quite encouraging.

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Adjusted R\textsuperscript{2} values for the nine Stochastic OT models of object drop in English.}
\labtab{tab_adjrsq_eng}
\begin{tabular}{l|ccc}
& SPS & Comp PISA & Behav PISA \\
\hline
StOT basic           & 0.422        & 0.457     & 0.467      \\
StOT +iter           & 0.421        & 0.456     & 0.466      \\
StOT +iter +spec     & 0.425        & 0.454     & 0.468  
\end{tabular}
\end{table}

Let us inspect \reftab{tab_adjrsq_eng} in more detail. Looking at it horizontally, i.e. comparing the performance of each type of model (basic, intermediate, full) on varying the measure of semantic selectivity, it emerges that models using Behavioral PISA have a better explanatory power than models using Computational PISA, which in turn are better than models using Resnik's SPS, regardless of the number of predictors in the model. Looking at the table vertically, i.e. comparing the performance of the three increasingly rich models of object drop based on the same measure of semantic selectivity, it results that the intermediate model is consistently worse (albeit imperceptibly) than the basic model regardless of the measure of semantic selectivity, while the addition of manner specification as a predictor in the full model makes it a better fit when using Resnik's SPS and Behavioral PISA, but a slightly worse fit than the intermediate model when using Computational PISA. Moreover, consistently with conclusions drawn in \refsec{eng_judgresult_semsel}, the difference in performance between the models based on Computational PISA and those based on Behavioral PISA is much lower than the difference between either of those and the models based on Resnik's SPS. In general, it is possible to conclude that a full, five-predictor model is a sensible choice to model the grammaticality of the implicit object construction in English, and the best model among the three full models is the one using Behavioral PISA to quantify semantic selectivity. I will provide a thorough analysis of this model in \refsec{stot_full}.

\paragraph{Italian} Let us now examine the performance of the nine Stochastic Optimality Theoretic models of object drop in Italian and compare it to the results for English I just discussed. The Pearson correlations between actual and predicted grammaticality judgments in \reftab{tab_pearson_ita}, all highly significant (p < 0.001), show varying degrees of reliability ranging from 0.621 for basic and intermediate models using Computational PISA to 0.694 for the full model using Resnik's SPS. As for English, I evaluated the goodness-of-fit of the nine models for Italian by computing the adjusted R\textsuperscript{2} values in \reftab{tab_adjrsq_ita}. These coefficients show that some models are a fairly poor fit (especially the intermediate model using Computational PISA, which only explains 36.5\% of the variance in the data), and only two of them have a performance comparable with the English models (the full model using Resnik's SPS explains 45.8\% of the variance, the full model using Behavioral PISA explains 45.5\%).

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Pearson correlations between actual and predicted values for the nine Stochastic OT models of object drop in Italian.}
\labtab{tab_pearson_ita}
\begin{tabular}{l|ccc}
& SPS & Comp PISA & Behav PISA \\
\hline
StOT basic           & 0.637        & 0.621     & 0.655      \\
StOT +iter           & 0.637        & 0.621     & 0.655      \\
StOT +iter +spec     & 0.694        & 0.655     & 0.692     
\end{tabular}
\end{table}

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Adjusted R\textsuperscript{2} values for the nine Stochastic OT models of object drop in Italian.}
\labtab{tab_adjrsq_ita}
\begin{tabular}{l|ccc}
& SPS & Comp PISA & Behav PISA \\
\hline
StOT basic           & 0.391        & 0.370     & 0.414      \\
StOT +iter           & 0.386        & 0.365     & 0.410      \\
StOT +iter +spec     & 0.458        & 0.404     & 0.455     
\end{tabular}
\end{table}

Taking a closer look at \reftab{tab_adjrsq_ita}, severals conclusions can be drawn. Looking at it line-by-line, it appears that Computational PISA-based models are consistently the worst for each type of Stochastic Optimality Theoretic model and Behavioral PISA-based models are the best (it actually loses to Resnik's SPS in the case of the full models, but only by a negligible 0.3\% difference). Looking at it column-by-column, results show that the intermediate model is consistently worse than the basic model, which in turn is consistently worse than the full model, indicating that iterativity alone is not a good addition to the basic three-predictor model devised by \textcite{Medina2007}, but iterativity and manner specification together provide the model with a much stronger explanatory power. Interestingly, there is a stark difference between the performance of Behavioral PISA-based models on one hand, and the performance of models using corpus-based measures of semantic selectivity (Resnik's SPS and Computational PISA) on the other hand. This state of affairs mirrors closely the conclusions I drew in \refsec{ita_judgresult_semsel} about the way these measures of semantic selectivity were computed, also in contrast with English results. All in all, it results that it makes sense to compute a five-predictor model to understand the factors regulating the implicit object construction in Italian, and it is best to implement semantic selectivity in such a model using Behavioral PISA despite the remarkable performance of the full SPS-based model (given all the drawbacks of Resnik's SPS which I pointed out throughout this Section and in \refch{results}).

\paragraph{Comparing English and Italian} By looking at \reftab{tab_adjrsq_eng} and \reftab{tab_adjrsq_ita} in particular, it is evident that any given model using the same set of predictors and the same measure of semantic selectivity fits English data better than Italian data, with this difference being way more noticeable for models employing corpus-based measures of semantic selectivity than for models using Behavioral PISA (based on human similarity ratings) for the same purpose. As observed multiple times here and in \refch{results}, this may be most likely due to the better overall quality of the ukWaC corpus I used to model English if compared to itWaC, a web-based corpus of Italian that is similar in nature and dimensions (refer back to \refsec{compuPisa} for more details on the two corpora).\\
Another intriguing difference between English and Italian relative to object drop that emerges by comparing \reftab{tab_adjrsq_eng} and \reftab{tab_adjrsq_ita} is that the addition of manner specification to the model determines a veritable qualitative leap in the case of Italian, where the full models are way better than the basic and intermediate ones, while the same is not true of English, where the performance of full models is quite similar (although slightly better) to that of basic and intermediate models. Given that all other factors in the models, as well as the stimuli used in the experiments, are identical in all respects but the language itself, it is clear that this difference between English and Italian models has to be ascribed to manner specification itself. We could be tempted to seek an explanation in the well-known distinction between verb-framed and satellite-framed languages proposed by \textcite{talmy1991path, talmy2000toward} with respect to motion verbs. Verb-framed languages, such as Italian, typically encode the Path of motion in the verb root, while they (optionally) express the manner of motion via additional lexical material (e.g. \textit{uscì correndo}, lit. 'he/she went-out running'). Satellite-framed languages, such as English, typically encode the manner of motion in the verb root and make use of particles to encode the Path of motion (e.g. \textit{to go in}, \textit{to fall down}). Looking at \reftab{tab_adjrsq_eng} and \reftab{tab_adjrsq_ita} through Talmy-styled lenses, it would seem that Italian speakers are much more sensitive to manner being encoded in the verb root than English speakers due to Italian being a verb-framed language, despite there being no framing-dependent differences in the surface form of the verbs used in the stimuli (e.g. Eng. \textit{to devour} / It. \textit{divorare}). On the flip side, the distinction between verb- and satellite-framed languages seems to have much less hold outside the domain of motion verbs (see \textcite{mastrofini2013english} about manner-of-speaking verbs in English and Italian), and therefore it is possible that the explanation for the spike in the Italian (and not in the English) adjusted R\textsuperscript{2} values due to the manner specification parameter has to be found elsewhere.


\section{A full account of the full models} \labsec{stot_full}
In this Section, I will only discuss two of the 18 models in \reftab{tab_mymodels}, namely the best-performing model of object drop in English and the best model for Italian. It would be impossible to dedicate the same amount of attention to all the models I computed, but the interested reader can find a summary of the relevant results in \refapp{app_models}.\\
Based on \reftab{tab_adjrsq_eng}, the best-performing model of the implicit object construction in English is the full model making use of Behavioral PISA to measure semantic selectivity. As for Italian, I would have to choose the full model quantifying semantic selectivity with Resnik's SPS based on the results in \reftab{tab_adjrsq_ita}, but I will instead present and discuss the full model using Behavioral PISA thanks to the negligible R\textsuperscript{2} difference between this model and the one using Resnik's SPS in Italian. Crucially, this choice will make it possible to compare the English and the Italian models, since it minimizes the differences between them.


\subsection{A quick recap} \labsec{stot_full_recap}

I will now go briefly over the logic behind Medina's variant of Stochastic Optimality Theory (first introduced in \refpage{3steplogicmedina}) which I am using to compute the full models of the gradient grammaticality of object drop in English and Italian. In a nutshell,

\begin{enumerate}
    \item the probability of \textsc{*Int Arg} dominating each of the other constraints\sidenote{\textsc{*Int Arg} has to be ranked above all the other active constraints for a given input in order to obtain an implicit object output (see \refsec{constraintsmedina} and \refsec{intro_constraints})} is expressed as a function of the input verb's semantic selectivity (which I compute using Resnik's SPS, Computational PISA, and Behavioral PISA);
    \item the values of the function are used to compute the relative probabilities of each of the 16 possible re-rankings of \textsc{*Int Arg} with respect to the five other constraints at play (see \reftab{me_16rankings})\sidenote{In order to save space, in \reftab{me_16rankings} \textsc{Faith Arg}, \textsc{Telic End}, \textsc{Perf Coda}, \textsc{Non-Iter Arg}, and \textsc{Mann-Spec Arg} are referred to as *I, F, T, P, N, and M, respectively.};
    \item these relative probabilities determine the relative probability (and thus grammaticality) of the implicit object output for a given input, depending on the input's semantic selectivity score and binary aspectual features.
\end{enumerate}

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Set of the 16 possible re-rankings of \textsc{*Int Arg} with respect to \textsc{Faith Arg}, \textsc{Telic End}, \textsc{Perf Coda}, \textsc{Non-Iter Arg}, and \textsc{Mann-Spec Arg}, these being unordered with respect one to another.}
\labtab{me_16rankings} 
\begin{tabular}{c}
\textsc{*Int Arg} $\gg$ \{F, T, P, N, M\} \\
T $\gg$ \textsc{*Int Arg} $\gg$ \{F, P, N, M\} \\
P $\gg$ \textsc{*Int Arg} $\gg$ \{F, T, N, M\} \\
\{T, P\} $\gg$ \textsc{*Int Arg} $\gg$ \{F, N, M\} \\
M $\gg$ \textsc{*Int Arg} $\gg$ \{F, T, P, N\} \\
\{M, T\} $\gg$ \textsc{*Int Arg} $\gg$ \{F, P, N\} \\
\{M, P\} $\gg$ \textsc{*Int Arg} $\gg$ \{F, T, N\} \\
\{M, T, P\} $\gg$ \textsc{*Int Arg} $\gg$ \{F, N\} \\
N $\gg$ \textsc{*Int Arg} $\gg$ \{F, T, P, M\} \\
\{N, T\} $\gg$ \textsc{*Int Arg} $\gg$ \{F, P, M\} \\
\{N, P\} $\gg$ \textsc{*Int Arg} $\gg$ \{F, T, M\} \\
\{N, T, P\} $\gg$ \textsc{*Int Arg} $\gg$ \{F, M\} \\
\{M, N\} $\gg$ \textsc{*Int Arg} $\gg$ \{F, T, P\} \\
\{M, N, T\} $\gg$ \textsc{*Int Arg} $\gg$ \{F, P\} \\
\{M, N, P\} $\gg$ \textsc{*Int Arg} $\gg$ \{F, T\} \\
\{M, N, T, P\} $\gg$ \textsc{*Int Arg} $\gg$ F \\
\end{tabular}
\end{table}

As shown in \reftab{medina_nicetable}, the actual computational steps needed to model object drop go backwards with respect to the three-step logic I summed up just now. Recalling the summary of Medina's computational reasoning in \refpage{3stepcompmedina}, I will follow the exact same procedure:
\begin{enumerate}
    \item the grammaticality of the indefinite object drop is quantified via an acceptability judgment survey (refer back to \refch{judgments} for the experimental setting and to \refch{results} for the results), the results thereof are equated to the probability of an implicit object output for a given input;
    \item the probability of each of the 16 possible constraint orderings in \reftab{me_16rankings} can be estimated via the probability of an implicit object output (i.e. the average judgment for a given input, normalized between 0 and 1);
    \item knowing the probability of each constraint ordering, it is possible to estimate the probability of \textsc{*Int Arg} dominating each constraint and, finally, the probability of obtaining an implicit object output with each type of input.
\end{enumerate}

In particular, I will assess the mathematical procedure in \refsec{stot_full_fitting}, while I will discuss the probability of \textsc{*Int Arg} dominating each of the other five constraints and the probability of each type of input resulting in an implicit object output in \refsec{stot_full_parameters} and \refsec{stot_full_predicted}, respectively.


\subsection{Fitting the model} \labsec{stot_full_fitting}

In the full Stochastic Optimality Theoretic model(s) I am going to compute, based on my four binary predictors, there are 16 different types of input. The combinatory logic behind this result is shown in \reftab{me_16inputs}.

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{The four binary constraints in the full Stochastic Optimality Theoretic model give rise to 16 different types of inputs.}
\labtab{me_16inputs} 
\begin{tabular}{l|cccc}
         & telicity & perfectivity & iterativity & manner specification \\
         \hline
input 1  & +       & +           & +          & +                   \\
input 2  & +       & +           & +          & -                   \\
input 3  & +       & +           & -          & +                   \\
input 4  & +       & +           & -          & -                   \\
input 5  & +       & -           & +          & +                   \\
input 6  & +       & -           & +          & -                   \\
input 7  & +       & -           & -          & +                   \\
input 8  & +       & -           & -          & -                   \\
input 9  & -       & +           & +          & +                   \\
input 10 & -       & +           & +          & -                   \\
input 11 & -       & +           & -          & +                   \\
input 12 & -       & +           & -          & -                   \\
input 13 & -       & -           & +          & +                   \\
input 14 & -       & -           & +          & -                   \\
input 15 & -       & -           & -          & +                   \\
input 16 & -       & -           & -          & -           
\end{tabular}
\end{table}

As stated in \refsec{stot_full_recap}, the probability of an implicit object output for each type of input in \reftab{me_16inputs} is equal to the normalized acceptability rating attributed to that specific input. Then, this rating-as-probability is equated to the joint probability of all the rankings in \reftab{me_16rankings} where \textsc{*Int Arg} is ranked above all the relevant, active constraints for the specific type of input under consideration. So, for instance, the probability of an implicit object output for a telic, perfective, non-iterative, manner-specified input is computed as in \refeq{probz1}, since this input violates all the five faithfulness constraints at play, making it necessary to have \textsc{*Int Arg} outranking all of them for an implicit object output to be licensed by the model. The probability of an implicit object output for an atelic, perfective, non-iterative, manner-specified input, instead, is computed as in \refeq{probz2} because the \textsc{Telic End} constraint is vacuously satisfied by atelic inputs. For the same reason, the probability of an implicit object output for atelic, imperfective, iterative, manner-unspecified inputs is computed as in \refeq{probz3}, i.e. as the joint probability of all the rankings in \reftab{me_16rankings} (since these inputs vacuously satisfy all the constraints in the model with the exception of \textsc{Faith Arg}).

\begin{align}  \labeq{probz1}
    & p(\text{implicit})\textsubscript{Tel Perf Non-Iter Spec} = p(*I \gg {F, T, P, N, M}) \\
    & p(\text{implicit})\textsubscript{Atel Perf Non-Iter Spec} = p(*I \gg {F, T, P, N, M}) + \nonumber \\ & + p(T \gg *I \gg {F, P, N, M}) \labeq{probz2}\\
    & p(\text{implicit})\textsubscript{Atel Imperf Iter Non-Spec} = p(*I \gg {F, T, P, N, M}) + \nonumber \\ & + p(T \gg *I \gg {F, P, N, M}) + p(P \gg *I \gg {F, T, N, M}) + \nonumber \\ & + p({T, P} \gg *I \gg {F, N, M}) + p(M \gg *I \gg {F, T, P, N}) + \nonumber \\ & + p({M, T} \gg *I \gg {F, P, N}) + p({M, P} \gg *I \gg {F, T, N}) + \nonumber \\ & + p({M, T, P} \gg *I \gg {F, N}) + p(N \gg *I \gg {F, T, P, M}) + \nonumber \\ & + p({N, T} \gg *I \gg {F, P, M}) + p({N, P} \gg *I \gg {F, T, M}) + \nonumber \\ & + p({N, T, P} \gg *I \gg {F, M}) + p({M, N} \gg *I \gg {F, T, P}) + \nonumber \\ & + p({M, N, T} \gg *I \gg {F, P}) + p({M, N, P} \gg *I \gg {F, T}) + \nonumber \\ & + p({M, N, T, P} \gg *I \gg F) \labeq{probz3}
\end{align}

Knowing the probability of an implicit object output for each type of input (i.e. the normalized judgment for that type of input), and knowing the computation of the joint probabilities of relative rankings which give rise to it (as just shown briefly in \labeq{probz1} to \labeq{probz3}), it is now possible to compute the probability of \textsc{*Int Arg} dominating \textit{each} of the other five constraints, which will be used later to determine the parameters of the model itself. Limiting my examples to two types of input to avoid encumbering the reader with unnecessary details, the probability of an implicit object output for a telic, perfective, non-iterative, manner-specified input (computed before in \refeq{probz1}) can be unpacked as in \refeq{probz1_loose}, while the probability of an implicit object output for an atelic, perfective, non-iterative, manner-specified input (computed before in \refeq{probz2}) can be unpacked as in \refeq{probz2_loose}.

\begin{align}  \labeq{probz1_loose}
    & p(\text{implicit})\textsubscript{Tel Perf Non-Iter Spec} = p(*I \gg F) \cdot p(*I \gg T) \cdot p(*I \gg P) \cdot \nonumber \\ & \cdot p(*I \gg N) \cdot p(*I \gg M) \\
    & p(\text{implicit})\textsubscript{Atel Perf Non-Iter Spec} = p(*I \gg F) \cdot p(*I \gg T) \cdot p(*I \gg P) \cdot \nonumber \\ & \cdot p(*I \gg N) \cdot p(*I \gg M) + p(*I \gg F) \cdot [1 - p(*I \gg T)] \cdot \nonumber \\ & \cdot p(*I \gg P) \cdot p(*I \gg N) \cdot p(*I \gg M) \labeq{probz2_loose}
\end{align}

As introduced in \refsec{rankingmedina}, the main innovation by \textcite{Medina2007} within the landscape of Stochastic Optimality Theory is the definition of the ranking of \textsc{*Int Arg} with respect to each of the other constraints at play as a (linear) function of the input verb's semantic selectivity. Such a function takes the shape of \refeq{medinafunction_ex_repeat} (updating \refeq{medinafunction_ex} with the use of Behavioral PISA instead of Resnik's SPS).

\begin{equation} \labeq{medinafunction_ex_repeat}
p(\textsc{*Int Arg} \gg \textrm{con}) = \frac{\delta_k - \gamma_k}{\textrm{bPISA}_{max} - \textrm{bPISA}_{min}} \cdot ({\textrm{bPISA}_{i} - \textrm{bPISA}_{min}}) + \gamma_k
\end{equation}

In particular, the five linear functions involved in my full models are shown in \refeq{medinafunction_f_repeat} to \refeq{medinafunction_m} (\refeq{medinafunction_f_repeat} to \refeq{medinafunction_p_repeat} are Medina's original \refeq{medinafunction_f} to \refeq{medinafunction_p}).

\begin{equation} \labeq{medinafunction_f_repeat}
p(\textsc{*Int Arg} \gg \textsc{F}) = \frac{\delta_1 - \gamma_1}{\textrm{bPISA}_{max} - \textrm{bPISA}_{min}} \cdot ({\textrm{bPISA}_{i} - \textrm{bPISA}_{min}}) + \gamma_1
\end{equation}

\begin{equation} \labeq{medinafunction_t_repeat}
p(\textsc{*Int Arg} \gg \textsc{T}) = \frac{\delta_2 - \gamma_2}{\textrm{bPISA}_{max} - \textrm{bPISA}_{min}} \cdot ({\textrm{bPISA}_{i} - \textrm{bPISA}_{min}}) + \gamma_2
\end{equation}

\begin{equation} \labeq{medinafunction_p_repeat}
p(\textsc{*Int Arg} \gg \textsc{P}) = \frac{\delta_3 - \gamma_3}{\textrm{bPISA}_{max} - \textrm{bPISA}_{min}} \cdot ({\textrm{bPISA}_{i} - \textrm{bPISA}_{min}}) + \gamma_3
\end{equation}

\begin{equation} \labeq{medinafunction_n}
p(\textsc{*Int Arg} \gg \textsc{N}) = \frac{\delta_4 - \gamma_4}{\textrm{bPISA}_{max} - \textrm{bPISA}_{min}} \cdot ({\textrm{bPISA}_{i} - \textrm{bPISA}_{min}}) + \gamma_4
\end{equation}

\begin{equation} \labeq{medinafunction_m}
p(\textsc{*Int Arg} \gg \textsc{M}) = \frac{\delta_5 - \gamma_5}{\textrm{bPISA}_{max} - \textrm{bPISA}_{min}} \cdot ({\textrm{bPISA}_{i} - \textrm{bPISA}_{min}}) + \gamma_5
\end{equation}

It is now possible to compute the probability of an implicit object output for any type of input in terms of a polynomial function computed as the product of several linear functions whose independent variable is the verb's Behavioral PISA score. This result can be obtained by plugging \refeq{medinafunction_f_repeat} to \refeq{medinafunction_m} into the computations of the joint probabilities of \textsc{*Int Arg} dominating each of the other five constraints. So, for instance, the probability of an implicit object output for a telic, perfective, non-iterative, manner-specified input (\refeq{probz1_loose}) can be computed as in  \refeq{polinomial_tpnm}.

\begin{align}  \labeq{polinomial_tpnm}
    & p(\text{implicit})\textsubscript{Tel Perf Non-Iter Spec} = \nonumber \\ & = [\frac{\delta_1 - \gamma_1}{\textrm{bPISA}_{max} - \textrm{bPISA}_{min}} \cdot ({\textrm{bPISA}_{i} - \textrm{bPISA}_{min}}) + \gamma_1] \cdot \nonumber \\ & \cdot [\frac{\delta_2 - \gamma_2}{\textrm{bPISA}_{max} - \textrm{bPISA}_{min}} \cdot ({\textrm{bPISA}_{i} - \textrm{bPISA}_{min}}) + \gamma_2] \cdot \nonumber \\ & \cdot [\frac{\delta_3 - \gamma_3}{\textrm{bPISA}_{max} - \textrm{bPISA}_{min}} \cdot ({\textrm{bPISA}_{i} - \textrm{bPISA}_{min}}) + \gamma_3] \cdot \nonumber \\ & \cdot [\frac{\delta_4 - \gamma_4}{\textrm{bPISA}_{max} - \textrm{bPISA}_{min}} \cdot ({\textrm{bPISA}_{i} - \textrm{bPISA}_{min}}) + \gamma_4] \cdot \nonumber \\ & \cdot [\frac{\delta_5 - \gamma_5}{\textrm{bPISA}_{max} - \textrm{bPISA}_{min}} \cdot ({\textrm{bPISA}_{i} - \textrm{bPISA}_{min}}) + \gamma_5]
\end{align}

All the variables in such equations are known, with the exception of $\gamma$s and $\delta$s, which are the values the linear functions take at $\textrm{bPISA}_{min}$ and $\textrm{bPISA}_{max}$, respectively. Based on Medina's method (see \refpage{medinaexcelsolver}), the computational model of the indefinite object construction takes as input the acceptability judgments (normalized between 0 and 1) and the Behavioral PISA scores for all the target stimuli, and optimizes the relevant polynomial functions (such as the one in \refeq{polinomial_tpnm}) so that:

\begin{itemize}
    \item $\delta_i$ and $\gamma_i$ fall between 0 and 1
    \item the Summed Squared Error\sidenote{Refer to \refeq{sse} for a definition of Summed Squared Error.} between the actual judgments and the ones predicted by the model have to be minimized
\end{itemize}

PYTHON

% alla fine della sezione, dire che tutto questo l'ho fatto con uno script in python che sta sul mio github


\subsection{Parameters of the linear functions} \labsec{stot_full_parameters}

quei 3/5 plot con la righina unica come Medina (ha senso metterli tutti nello stesso plot?)
riportare anche la tabella coi valori numerici E INDICARE LE INTERSEZIONI PER IL RUOLO DI SPS

confrontare coi risultati originali di Medina! qui posso certamente farlo (oppure scrivo proprio un capitoletto a parte, tra "the input the constraints the output" e "model evaluation", dedicato al confronto con Medina e commento tutto il commentabile: Medina vs basic-SPS-Eng mio) + correggere le parti qui dentro dove dicevo "oddly enough..."

\paragraph{English} testo

\begin{figure}[htb]
\caption{Probability of \textsc{*Int Arg} being ranked above each of the other constraints, varying in accordance with Behavioral PISA (English full model).}
\labfig{eng_ext2_bpisa_alltogether}
    \input figures/eng_ext2_bpisa_prob_alltogether.tex
\end{figure}

\paragraph{Italian} testo

\begin{figure}[htb]
\caption{Probability of \textsc{*Int Arg} being ranked above each of the other constraints, varying in accordance with Behavioral PISA (Italian full model).}
\labfig{ita_ext2_bpisa_alltogether}
    \input figures/ita_ext2_bpisa_prob_alltogether.tex
\end{figure}


\subsection{Predicted grammaticality of an implicit object output} \labsec{stot_full_predicted}

il plot con le cubiche/polinomiali E INDICARE LE INTERSEZIONI PER IL RUOLO DI SPS

\paragraph{English} testo

\begin{figure}[htb]
\caption{Probability of an implicit object output for each aspectual type, as a function of Behavioral PISA (English full model).}
\labfig{eng_ext2_bpisa_aspectualtypes}
    \input figures/eng_ext2_bpisa_prob_aspectualtypes.tex
\end{figure}

\paragraph{Italian} testo

\begin{figure}[htb]
\caption{Probability of an implicit object output for each aspectual type, as a function of Behavioral PISA (Italian full model).}
\labfig{ita_ext2_bpisa_aspectualtypes}
    \input figures/ita_ext2_bpisa_prob_aspectualtypes.tex
\end{figure}


\subsection{Model assessment} \labsec{stot_full_assessment}

adjusted R squared + 
individual error (Medina 146) +
Pearson actual-predicted collettivo e per ciascun aspectual type
(criticism of Medina per la scelta di fare la somma degli squared errors)

\paragraph{English} testo

\paragraph{Italian} testo


\subsection{Comparing the English and Italian models} \labsec{stot_full_engita}

testo



\section{Final remarks} \labsec{stot_conclusions}


\subsection{Gradience} \labsec{concl_gradient}

model of GRADIENT grammaticality
object drop may be impossible (telic perf?), obligatory (atelic imperf?), but it is generally optional in degrees as shown in the model


\subsection{Is the best model a good model?} \labsec{concl_aspectualtypes}

parlare dei risultati dei singoli tipi aspettuali (Pearson) e vedere dove fallisce o meno


\subsection{On regression models} \labsec{concl_lmem}

confrontare i risultati col modello lmem fatto nel capitolo precedente e vedere se effettivamente un modello di regressione può essere usato come modello linguistico
