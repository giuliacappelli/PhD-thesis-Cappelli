% \setchapterstyle{kao}
\setchapterpreamble[u]{\margintoc}
\chapter{Predicting the grammaticality of implicit objects}
\labch{model}

\section{Introduction} \labsec{introfitting}

In this Chapter I will model the grammaticality of the implicit object construction (\refch{indefinitedrop}) in a Stochastic Optimality Theoretic fashion inspired by \textcite{Medina2007} (\refch{modeltheory}), using five aspectual and semantic factors (\refch{factors} and \refch{predictors}) as constraints in several models of human acceptability judgments (\refch{judgments}) in English and Italian. Based on the results of these two behavioral experiments described in \refch{results}, a linguistically-motivated probabilistic model of object drop considering the joint effect of all five predictors is indeed feasible.\\
In particular, I will describe the models in general in \refsec{intro_models}, I will delve into the finer details of the full English and Italian models of object drop as a function of Behavioral PISA (introduced in \refsec{behavPisa}) in \refsec{stot_full}, and I will draw some conclusions about relevant linguistic and mathematical aspects of these models in \refsec{stot_conclusions}.

\subsection{Progressive models} \labsec{intro_models}

In this thesis, I build upon the foundations laid by \textcite{Medina2007}, which I detailed in \refsec{medinamodel}. In a nutshell, her Stochastic Optimality Theoretic analysis of the implicit object construction was focused on English, and used a set of only three predictors (semantic selectivity, telicity, and perfectivity) as constraints in the model. Moreover, she measured the verbs' semantic selectivity using the Selectional Preference Strength values originally computed by \textcite{Resnik1993,Resnik1996}, which poses evident limitations in the choice of transitive verbs to include in the model and which also suffers from some computational drawbacks due to being a taxonomy-based measure (more on this in \refsec{evalMySPSs}).\\
Expanding on Medina's successful model of object drop, I bring several new ideas to the table:
\begin{itemize}
    \item quantifying semantic selectivity with two similarity-based measures, i.e. a novel computational one I contributed to develop in \textcite{CappelliLenciPISA} (Computational PISA, see \refsec{compuPisa}), and a behavioral one that improves on Medina's measure of Object Similarity (Behavioral PISA, see \refsec{behavPisa});
    \item modeling the implicit object construction both in English and in Italian, comparing the performance of the two models and possible language-dependent differences in the constraint re-ranking;
    \item computing increasingly more complex Stochastic Optimality Theoretic models of object drop, starting with Medina's three-predictor one, adding iterativity as a predictor in an intermediate model, and computing the full five-predictor model also including manner specification among the predictors.
\end{itemize}
These additions to Medina's setting resulted in a grand total of 18 models of the implicit object construction, which are summarized in \reftab{tab_mymodels} for the reader's convenience.

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{The 18 Stochastic Optimality Theoretic models of object drop I computed for English and Italian.}
\labtab{tab_mymodels}
\begin{tabular}{l|ccc}
& SPS & Comp PISA & Behav PISA \\
\hline
StOT basic & eng | ita          & eng | ita   & eng | ita   \\
StOT +iter & eng | ita  & eng | ita & eng | ita \\
StOT +iter +spec & eng | ita   & eng | ita   & eng | ita  
\end{tabular}
\end{table}

The basic Stochastic Optimality Theoretic model of English judgments using Resnik's SPS as a measure of semantic selectivity is, as recalled earlier in this Section, a reproduction of the model by \textcite{Medina2007} employing the same constraints and acceptability judgments based on the same experimental protocol (but with different target verbs and an updated computational preprocessing pipeline, as explained in \refch{judgments} and \refch{results}). The other 17 models are instead first-time analyses.\\
Naturally, one could ask why it is iterativity, and not manner specification, the predictor of object drop to be included in the intermediate Stochastic Optimality Theoretic models. After all, the main effect of iterativity was shown to be non-significant in \reffig{explore_eng_iterativity} and \reffig{explore_ita_iterativity} both for English and Italian, unlike the very significant main effect of manner specification in both languages (refer back to \reffig{explore_eng_mannspec} and \reffig{explore_ita_mannspec}). Crucially though, I am creating probabilistic models considering the \textit{joint} effect of five linguistic factors on the grammaticality of the implicit object construction. Since the linear mixed-effects models for English (see \refsec{eng_jointeffect}) revealed a highly significant effect of iterativity and a non-significant effect of manner specification, while the two predictors were almost equally non-significant in the mixed models for Italian (see \refsec{ita_jointeffect}), it appeared that iterativity plays a larger role in determining the grammaticality of object drop when considered in combination with all the other linguistic factors involved.


\subsection{The input, the output, and the constraints} \labsec{intro_constraints}

A very short summary of the lengthy explanation of (Stochastic) Optimality Theory I provided in \refch{modeltheory}, and especially of the explanation of the novel variant by \textcite{Medina2007} in \refsec{medinamodel}, is in order. In particular, I am going to retrace the way the input to the optimization process maps to the output, and I will introduce my two novel constraints after looking back on Medina's original set.\\
As shown in \ref{medina_input_generic} within \refsec{inputmedina}, the input to the syntactic optimization operated by the model has to include all the relevant lexical and semantic information that will be mapped to syntactically well-formed output forms, and nothing else. Thus, the input to my basic Stochastic Optimality Theoretic models (and Medina's) will look like \ref{my_input_basic}, the input to the intermediate models will look like \ref{my_input_ext1}, and the input to the full models with all five predictors will look like \ref{my_input_ext2}. All inputs in \ref{my_input_generic} contain a transitive verb with a subject and an unspecified direct object (since the model deals with indefinite, not definite, object drop), a numerical value for semantic selectivity (be it Resnik's SPS, Computational PISA, or Behavioral PISA), the [+Past] feature since all verbs in the stimuli are in the past tense, and the features of the predicate relative to all the binary predictors that are relevant in the model (two in the basic model, three in the intermediate model, four in the full model).

\ex. \label{my_input_generic} 
\a. \label{my_input_basic} verb (x,y), x = subject, y = unspecified, semantic selectivity = \textit{numerical value}, [+ Past], [$\pm$ Telic], [$\pm$ Perfective]
\b. \label{my_input_ext1} verb (x,y), x = subject, y = unspecified, semantic selectivity = \textit{numerical value}, [+ Past], [$\pm$ Telic], [$\pm$ Perfective], [$\pm$ Iterative]
\c. \label{my_input_ext2} verb (x,y), x = subject, y = unspecified, semantic selectivity = \textit{numerical value}, [+ Past], [$\pm$ Telic], [$\pm$ Perfective], [$\pm$ Iterative], [$\pm$ Manner Specified]

Given these inputs, the \textsc{Gen} component of the Optimality Theoretic grammar (see \refsec{classicot}) generates two outputs, i.e. one with an overt (unspecified) direct object and one with an implicit direct object.\\
Now that the grammar yielded a complete candidate set to evaluate, the model has to pick a winner (or, in our case, assign gradient grammaticality to the implicit object output in a probability space) based on the re-ranking of the relevant constraints. For the basic model, these are the ones in \ref{my_constraints_medina} (adapted from Medina's ones in \ref{medina_constraints}, introduced in \refsec{constraintsmedina}).

\ex. \label{my_constraints_medina} 
\a. \label{my_constraints_intarg} \textsc{*Int Arg (*Internal Argument Structure)}\\ The output must not contain an overt direct object.
\b. \label{my_constraints_faith} \textsc{Faith Arg (Faithfulness to Argument Structure)}\\ All arguments in the input must be present in the output.
\c. \label{my_constraints_telic} \textsc{Telic End (Telic Endpoint)}\\ Telic predicates must be bounded by an object in the output.
\c. \label{my_constraints_perf} \textsc{Perf Coda (Perfective Coda)}\\ Perfective predicates must have a direct object in the output.

I also designed the two novel constraints in \ref{my_constraints_new}, based on theoretical observations on iterativity and manner specification first introduced in \refch{factors} and explored further in \refch{predictors}. \textsc{Non-Iter Arg} is active both in the intermediate and in the full model, while \textsc{Mann-Spec Arg} is only active in the full model.

\ex. \label{my_constraints_new} 
\a. \label{my_constraints_iter} \textsc{Non-Iter Arg (Non-Iterative Argument)}\\ Non-iterative predicates must occur with a direct object in the output.
\b. \label{my_constraints_spec} \textsc{Mann-Spec Arg (Manner-Specified Argument)}\\ Manner-specified verbs must occur with a direct object in the output.

In all my Stochastic Optimality Theoretic models, \textsc{*Int Arg} is a markedness constraint that gets violated when there is an overt direct object in the output, directly conflicting with all the other constraints, which are \textit{faithfulness} constraints penalizing implicit objects. This conflict between markedness and faithfulness constraints is the very core of an Optimality Theoretic grammar (refer back to \refch{modeltheory}).\\
In the specific case of this thesis, an implicit object output will be (probabilistically) grammatical whenever \textsc{*Int Arg} is ranked above all the other constraints that are active (i.e. not vacuously satisfied\sidenote{As explained in detail in \refsec{constraintsmedina}, a constraint is vacuously satisfied when no candidate in the candidate set can violate it.}) for a given input. For instance, the full, five-predictor model would only favor object-dropping telic, perfective, non-iterative, manner-specified candidates if \textsc{*Int Arg} was ranked above \textsc{Faith Arg}, \textsc{Telic End}, \textsc{Perf Coda}, \textsc{Non-Iter Arg}, and \textsc{Mann-Spec Arg}. The same model would only require \textsc{*Int Arg} to be ranked above \textsc{Faith Arg} to allow for object drop in atelic, imperfective, iterative, manner non-specified candidates.

% non-iter arg è formulato in modo "negativo" perché altrimenti sarebbe un markedness constraint (è vero? cambierebbe qualcosa?)


\subsection{Model evaluation} \labsec{intro_evaluation}

Let us put aside the inner workings of Medina-inspired Stochastic Optimality Theoretic models for the time being, and let us consider whether increasing the number of predictors (and thus the number of constraints) actually determines a better understanding of the nature of the implicit object construction. I am going to come back to the mathematical details of the model in \refsec{stot_full}, where I will focus on the two best-performing models, one for English and one for Italian. Unfortunately, it would be impossible to provide a complete account of all 18 models in \reftab{tab_mymodels} due to space constraints, but the interested reader can find graphical summaries of their results in \refapp{app_models}.

\paragraph{English} An initial step to assess the absolute performance of each model, and hence to compare them and gauge their performance relative to each other, would be to compute Pearson correlations\sidenote{The Pearson's $\rho$ coefficient measuring the correlation between two variables can vary between -1 and 1. The strength of the correlation is judged by considering the absolute value of the coefficient, so that it is non-existent when $\rho$ is 0, very strong when it is closer to (-)1. If $\rho$ is positive the two variables are directly proportional one to another, while if it is negative they are indirectly proportional.} between the actual acceptability judgments provided by native speakers and the predicted grammaticality values yielded by each model. The Pearson's $\rho$ coefficients for English, all highly significant (p < 0.001), are collected in \reftab{tab_pearson_eng}. These results show that the predicted values correlate quite well with the human-generated values in each model, going from 0.661 for the basic model using Resnik's SPS as a measure of semantic selectivity to 0.700 for the full model using Behavioral PISA as a measure of semantic selectivity.

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Pearson correlations between actual and predicted values for the nine Stochastic OT models of object drop in English.}
\labtab{tab_pearson_eng}
\begin{tabular}{l|ccc}
& SPS & Comp PISA & Behav PISA \\
\hline
StOT basic           & 0.661        & 0.686     & 0.693      \\
StOT +iter           & 0.664        & 0.689     & 0.696      \\
StOT +iter +spec     & 0.670        & 0.691     & 0.700     
\end{tabular}
\end{table}

However, the correlation coefficient only serves to quantify the strength of the linear relationship between actual and predicted judgments, without providing any information on how well the independent variables in the model (i.e. the predictors) explain the variance in the dependent variable (i.e. the acceptability judgments). I gleaned this information by computing the adjusted R\textsuperscript{2} value for each model, obtaining the results in \reftab{tab_adjrsq_eng} relative to English.\\
The R-squared value, also known as "coefficient of determination", can be computed as the squared Pearson's $\rho$ or, alternatively, as in \refeq{rsquared} (with the Summed Squared Error computed as in \refeq{sse} and the Total Sum of Squares computed as in \refeq{tss}).

\begin{equation} \labeq{rsquared}
R^2 = 1 - \frac{\textrm{Summed Squared Error}}{\textrm{Total Sum of Squares}}
\end{equation}

The Summed Squared Error, computed with \refeq{sse}, is defined as the sum of the squared difference between each actual judgment and its corresponding judgment predicted by the model, for all judgments in the sample.

\begin{equation} \labeq{sse}
SSE = \sum_{i=1}^{n} (y_i - \widehat{y})^2
\end{equation}

The Total Sum of Squares is defined as the sum of the squared difference between each acceptability judgment in the sample and the average acceptability judgment, for all judgments in the sample. This is shown in \refeq{tss}.

\begin{equation} \labeq{tss}
TSS = \sum_{i=1}^{n} (y_i - \overline{y})^2
\end{equation}

R\textsuperscript{2} varies between 0 and 1, and it can be thought of as a percentage indicating the goodness of fit of a statistical model. However, it always increases when using additional predictors in a model, regardless of the usefulness of these variables in predicting the dependent variable. One could always add yet one more parameter to the model, overfit it to the data, and claim to have a very successful model due to a very high R\textsuperscript{2} value. In order to overcome this major drawback of R\textsuperscript{2}, it is recommended to compute an \textit{adjusted} R\textsuperscript{2} value that only increases when adding relevant parameters, while it decreases when adding useless ones. It is defined as in \refeq{adjR2}, where $n$ is the number of acceptability judgments to be predicted and $k$ is the number of independent variables (i.e. predictors) in the model, and it can be thought of as the percentage of variance explained by the sole indipendent variables that have an actual effect on the dependent variable.

\begin{equation} \labeq{adjR2}
\textrm{adjusted } R^2 = 1 - \frac{(1-R^2)(n-1)}{n-k-1}
\end{equation}

Let us close this much needed statistical parenthesis and go back to the summary of the English results. Oddly enough, \textcite[147]{Medina2007} limited her model assessment to the computation of the Summed Squared Error instead of also using it to compute the (adjusted) R\textsuperscript{2} value, which makes it impossible to compare mathematically her results and the ones I obtained in the basic model using Resnik's SPS as a measure of semantic selectivity. Moreover, the Summed Squared Error does not have an intrinsic meaning (unlike R\textsuperscript{2}), since it just increases whenever the total number of stimuli in the experiment increases (provided there is some difference between the actual and predicted values).\\
According to the adjusted R\textsuperscript{2} values in \reftab{tab_adjrsq_eng}, the nine models explain between 42.1\% (intermediate model with Resnik's SPS) and 46.8\% (full model with Behavioral PISA) of the variation in the data. Given the complex nature of the implicit object construction and the interaction between all the predictors, these results, though modest in absolute terms, are quite encouraging.

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Adjusted R\textsuperscript{2} values for the nine Stochastic OT models of object drop in English.}
\labtab{tab_adjrsq_eng}
\begin{tabular}{l|ccc}
& SPS & Comp PISA & Behav PISA \\
\hline
StOT basic           & 0.422        & 0.457     & 0.467      \\
StOT +iter           & 0.421        & 0.456     & 0.466      \\
StOT +iter +spec     & 0.425        & 0.454     & 0.468  
\end{tabular}
\end{table}

Let us inspect \reftab{tab_adjrsq_eng} in more detail. Looking at it horizontally, i.e. comparing the performance of each type of model (basic, intermediate, full) on varying the measure of semantic selectivity, it emerges that models using Behavioral PISA have a better explanatory power than models using Computational PISA, which in turn are better than models using Resnik's SPS, regardless of the number of predictors in the model. Looking at the table vertically, i.e. comparing the performance of the three increasingly rich models of object drop based on the same measure of semantic selectivity, it results that the intermediate model is consistently worse (albeit imperceptibly) than the basic model regardless of the measure of semantic selectivity, while the addition of manner specification as a predictor in the full model makes it a better fit when using Resnik's SPS and Behavioral PISA, but a slightly worse fit than the intermediate model when using Computational PISA. Moreover, consistently with conclusions drawn in \refsec{eng_judgresult_semsel}, the difference in performance between the models based on Computational PISA and those based on Behavioral PISA is much lower than the difference between either of those and the models based on Resnik's SPS. In general, it is possible to conclude that a full, five-predictor model is a sensible choice to model the grammaticality of the implicit object construction in English, and the best model among the three full models is the one using Behavioral PISA to quantify semantic selectivity. I will provide a thorough analysis of this model in \refsec{stot_full}.

\paragraph{Italian} Let us now examine the performance of the nine Stochastic Optimality Theoretic models of object drop in Italian and compare it to the results for English I just discussed. The Pearson correlations between actual and predicted grammaticality judgments in \reftab{tab_pearson_ita}, all highly significant (p < 0.001), show varying degrees of reliability ranging from 0.621 for basic and intermediate models using Computational PISA to 0.694 for the full model using Resnik's SPS. As for English, I evaluated the goodness-of-fit of the nine models for Italian by computing the adjusted R\textsuperscript{2} values in \reftab{tab_adjrsq_ita}. These coefficients show that some models are a fairly poor fit (especially the intermediate model using Computational PISA, which only explains 36.5\% of the variance in the data), and only two of them have a performance comparable with the English models (the full model using Resnik's SPS explains 45.8\% of the variance, the full model using Behavioral PISA explains 45.5\%).

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Pearson correlations between actual and predicted values for the nine Stochastic OT models of object drop in Italian.}
\labtab{tab_pearson_ita}
\begin{tabular}{l|ccc}
& SPS & Comp PISA & Behav PISA \\
\hline
StOT basic           & 0.637        & 0.621     & 0.655      \\
StOT +iter           & 0.637        & 0.621     & 0.655      \\
StOT +iter +spec     & 0.694        & 0.655     & 0.692     
\end{tabular}
\end{table}

\begin{table}[htb] % the "htb" makes table env unfloaty
\caption{Adjusted R\textsuperscript{2} values for the nine Stochastic OT models of object drop in Italian.}
\labtab{tab_adjrsq_ita}
\begin{tabular}{l|ccc}
& SPS & Comp PISA & Behav PISA \\
\hline
StOT basic           & 0.391        & 0.370     & 0.414      \\
StOT +iter           & 0.386        & 0.365     & 0.410      \\
StOT +iter +spec     & 0.458        & 0.404     & 0.455     
\end{tabular}
\end{table}

Taking a closer look at \reftab{tab_adjrsq_ita}, severals conclusions can be drawn. Looking at it line-by-line, it appears that Computational PISA-based models are consistently the worst for each type of Stochastic Optimality Theoretic model and Behavioral PISA-based models are the best (it actually loses to Resnik's SPS in the case of the full models, but only by a negligible 0.3\% difference). Looking at it column-by-column, results show that the intermediate model is consistently worse than the basic model, which in turn is consistently worse than the full model, indicating that iterativity alone is not a good addition to the basic three-predictor model devised by \textcite{Medina2007}, but iterativity and manner specification together provide the model with a much stronger explanatory power. Interestingly, there is a stark difference between the performance of Behavioral PISA-based models on one hand, and the performance of models using corpus-based measures of semantic selectivity (Resnik's SPS and Computational PISA) on the other hand. This state of affairs mirrors closely the conclusions I drew in \refsec{ita_judgresult_semsel} about the way these measures of semantic selectivity were computed, also in contrast with English results. All in all, it results that it makes sense to compute a five-predictor model to understand the factors regulating the implicit object construction in Italian, and it is best to implement semantic selectivity in such a model using Behavioral PISA despite the remarkable performance of the full SPS-based model (given all the drawbacks of Resnik's SPS which I pointed out throughout this Section and in \refch{results}).

\paragraph{Comparing English and Italian} By looking at \reftab{tab_adjrsq_eng} and \reftab{tab_adjrsq_ita} in particular, it is evident that any given model using the same set of predictors and the same measure of semantic selectivity fits English data better than Italian data, with this difference being way more noticeable for models employing corpus-based measures of semantic selectivity than for models using Behavioral PISA (based on human similarity ratings) for the same purpose. As observed multiple times here and in \refch{results}, this may be most likely due to the better overall quality of the ukWaC corpus I used to model English if compared to itWaC, a web-based corpus of Italian that is similar in nature and dimensions (refer back to \refsec{compuPisa} for more details on the two corpora).\\
Another intriguing difference between English and Italian relative to object drop that emerges by comparing \reftab{tab_adjrsq_eng} and \reftab{tab_adjrsq_ita} is that the addition of manner specification to the model determines a veritable qualitative leap in the case of Italian, where the full models are way better than the basic and intermediate ones, while the same is not true of English, where the performance of full models is quite similar (although slightly better) to that of basic and intermediate models. Given that all other factors in the models, as well as the stimuli used in the experiments, are identical in all respects but the language itself, it is clear that this difference between English and Italian models has to be ascribed to manner specification itself. A possible explanation could be found in the distinction between verb-framed and satellite-framed languages proposed by \textcite{talmy1991path}. 

% confrontare eng e ita (facendo riferimento all'evaluation dei modelli semsel e degli lmem)
% vedere quella cosa che diceva AL dei satellite/verb-framed cosi (qualitative leap)

% spiegare perché adesso parlo solo dei due full bPISA models


\section{Full Stochastic OT model} \labsec{stot_full}

% v. Medina 134-140, this section works backwards bla bla, guardare anche la mia slide che era piaciuta a Paul

\subsection{A quick recap} \labsec{stot_full_recap}

il discorso della logica che va al contrario dei computational steps


\subsection{Fitting the model} \labsec{stot_full_fitting}

estimation of unknown variables (equazioni e tutto)


\subsection{Parameters of the linear functions} \labsec{stot_full_parameters}

quei 3/5 plot con la righina unica come Medina (ha senso metterli tutti nello stesso plot?)
riportare anche la tabella coi valori numerici E INDICARE LE INTERSEZIONI PER IL RUOLO DI SPS

confrontare coi risultati originali di Medina! qui posso certamente farlo (oppure scrivo proprio un capitoletto a parte dedicato al confronto con Medina e commento tutto il commentabile)

\paragraph{English} testo

\begin{figure}[htb]
\caption{Probability of \textsc{*Int Arg} being ranked above each of the other constraints, varying in accordance with Behavioral PISA (English full model).}
\labfig{eng_ext2_bpisa_alltogether}
    \input figures/eng_ext2_bpisa_prob_alltogether.tex
\end{figure}

\paragraph{Italian} testo

\begin{figure}[htb]
\caption{Probability of \textsc{*Int Arg} being ranked above each of the other constraints, varying in accordance with Behavioral PISA (Italian full model).}
\labfig{ita_ext2_bpisa_alltogether}
    \input figures/ita_ext2_bpisa_prob_alltogether.tex
\end{figure}


\subsection{Predicted grammaticality of an implicit object output} \labsec{stot_full_predicted}

il plot con le cubiche/polinomiali E INDICARE LE INTERSEZIONI PER IL RUOLO DI SPS

\paragraph{English} testo

\begin{figure}[htb]
\caption{Probability of an implicit object output for each aspectual type, as a function of Behavioral PISA (English full model).}
\labfig{eng_ext2_bpisa_aspectualtypes}
    \input figures/eng_ext2_bpisa_prob_aspectualtypes.tex
\end{figure}

\paragraph{Italian} testo

\begin{figure}[htb]
\caption{Probability of an implicit object output for each aspectual type, as a function of Behavioral PISA (Italian full model).}
\labfig{ita_ext2_bpisa_aspectualtypes}
    \input figures/ita_ext2_bpisa_prob_aspectualtypes.tex
\end{figure}


\subsection{Model assessment} \labsec{stot_full_assessment}

adjusted R squared + 
individual error (Medina 146) +
Pearson actual-predicted collettivo e per ciascun aspectual type
(criticism of Medina per la scelta di fare la somma degli squared errors)

\paragraph{English} testo

\paragraph{Italian} testo


\subsection{Comparing the English and Italian models} \labsec{stot_full_engita}

testo



\section{Final remarks} \labsec{stot_conclusions}


\subsection{Gradience} \labsec{concl_gradient}

model of GRADIENT grammaticality
object drop may be impossible (telic perf?), obligatory (atelic imperf?), but it is generally optional in degrees as shown in the model


\subsection{Is the best model a good model?} \labsec{concl_aspectualtypes}

parlare dei risultati dei singoli tipi aspettuali (Pearson) e vedere dove fallisce o meno


\subsection{On regression models} \labsec{concl_lmem}

confrontare i risultati col modello lmem fatto nel capitolo precedente e vedere se effettivamente un modello di regressione può essere usato come modello linguistico
